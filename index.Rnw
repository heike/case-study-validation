\documentclass[doubleblind]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath,amsthm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{float}
% \usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}

\usepackage[numbers]{natbib}

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\journal{Forensic Science International}
%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
% \bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
% \bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num-names}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Comparison of three similarity scores for bullet LEA matching\tnoteref{t1}\tnoteref{t2}}

\tnotetext[t1]{This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.}

\tnotetext[t2]{The authors wish to thank Alan Zheng (NIST), Tylor Klep (Phoenix PD), and Melissa Nally and Kasi Kirksey (FSI Houston) for access to test set bullets. We would also like to thank the efforts of the Roy J Carver High Resolution lab in scanning the bullet sets and providing the scans to us.}

%% Group authors per affiliation:
% \author{Elsevier\fnref{myfootnote}}
% \address{Radarweg 29, Amsterdam}
% \fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
% \author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
% \ead[url]{www.elsevier.com}
% 
% \author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{support@elsevier.com}
% 
% \address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
% \address[mysecondaryaddress]{360 Park Avenue South, New York}

\author[isu,csafe]{Heike Hofmann}
\cortext[corauthor]{Corresponding author}\ead{srvander@iastate.edu}
\author[isu,csafe]{Susan Vanderplas\corref{corauthor}}
\address[isu]{Statistics Department, Iowa State University\\2438 Osborne Dr, Ames, IA 50011}
\address[csafe]{Center for Statistical Applications in Forensic Evidence, Iowa State University\\613 Morrill Rd, Ames, IA 50011}
   

\begin{abstract}
Recent advances in microscopy have made it possible to collect 3D topographic data, enabling virtual comparisons based on the collected 3D data as a supplement to traditional comparison microscopy and 2D photography. Automatic comparison algorithms have been introduced for various scenarios, such as matching cartridge cases \citep{tai2018fully,song3DTopographyMeasurements2014} or matching bullet striae \citep{aoas2, chuAutomaticIdentificationBullet2013, dekinderAutomatedComparisonsBullet1999}. One key aspect of validating these automatic comparison algorithms is to evaluate the performance of the algorithm on external tests, that is, using data which were not used to train the algorithm. Here, we present a discussion of the performance of the matching algorithm \citep{aoas2} in three studies conducted using different Ruger weapons. We consider the performance of three scoring measures: random forest score, cross correlation, and consecutive matching striae (CMS) at the land-to-land level and, using Sequential Average Maxima scores, also at the bullet-to bullet level. Cross correlation and random forest scores both result in perfect discrimination of same-source and different-source bullets. At the land-to-land level, discrimination for both cross correlation and random forest scores (based on area under the curve, AUC) is excellent ($\ge$ 0.90).
\end{abstract}

\begin{keyword}
forensic science, toolmark, cross correlation, random forest, 3D microscopy, land engraved areas (LEAs)
\end{keyword}

\end{frontmatter}



\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
\noindent

% \hh{
% Cleanup TO-DO
% \begin{itemize}
% \item \sout{define false positives and false negatives once and stick to those definitions. We should avoid false positive and false negatives, because that only makes sense in a test setting, which we have not defined formally.
% Let's use instead wrong identification and missed identification. }
% \item \sout{error rates: we are using rates exchangeably with percentages. We need to fix that. I think percentages might be easier understood. XXX Should be fixed now}
% \end{itemize}
% }
% \svp{
% Outstanding XXX items:
% \begin{itemize}
% \item ``Authors are responsible for obtaining written permission from persons acknowledged by name, because readers may infer their endorsement of the data and conclusions."
% \item thanks - scanning and hamby advice
% \item comparison to FTEs in conclusion
% \end{itemize}
% }


<<echo = FALSE, message = FALSE, warning = FALSE>>=
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  dpi = 600,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(stringr)

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletr")
library(gridExtra)
library(kableExtra)
@


 
In current practice, firearms and toolmark examiners (FTE) evaluate the similarity of striae on bullets by placing the evidence together with another bullet under a comparison microscope. The second bullet could be a test fire from a weapon recovered during the investigation, or it could be a second bullet from the crime scene. Examiners visually classify similarity according to the theory of firearms identification \citep{identification} as one of identification, inconclusive or exclusion. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citeauthor{biasotti} \citep{biasotti} is used as a measure to quantify the similarity of two lands. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization. 

% Identification using 3D Scanning Technology

More explicit characterization of bullet surfaces has been discussed since at least 1958 by \citeauthor{davis1968introduction} \cite{davis1968introduction}, but at the time technology was not sufficiently advanced to make an analysis based on 3D measurements or surface traces a viable option. \citet{ComputerIdentificationBullets1978} demonstrated use of a scanning electron microscope to quantitatively examine and compare bullet striae. More recently, approaches using 3D measurement data were explicitly described in 1999 by \citeauthor{dekinderAutomatedComparisonsBullet1999} \cite{dekinderAutomatedComparisonsBullet1999}, and have been further developed in \citep{bachrachDevelopment3DbasedAutomated2002,xieAutomatedBulletidentificationSystem2009, chuPilotStudyAutomated2010}. These approaches utilize 3D surface measurements directly to characterize the topology of land engraved areas (LEAs), rather than using visual or image comparison techniques. In many cases, these approaches also provide some level of automation of the comparison process, with the goal of reducing human biases by augmenting the visual information with 3D measurements. Utilizing the 3D measurements allows for examination of both peaks and valleys in LEAs, it also allows us to take the depth of striae into account, something that is difficult, if not impossible, from visual inspection.

Commonly, approaches derived from 3D surface measurements use some features which are similar to those visually assessed by FTEs \citep{luAutomatedBulletIdentification2014}. Class characteristics, which are shared by a group of firearms with the same rifling design, manufacturer, and tooling process, are typically evaluated first, as a mismatch on class charcteristics is sufficient for an exclusion. Automated approaches to estimation of width of the land engraved area and twist angle were described in \citet{chuPilotStudyAutomated2010}. Individual characteristics, which are not shared by all members of a class, can also be automatically assessed from 3D surface measurements. One of the most common features used to describe the similarity of two surfaces is the cross correlation function, which is utilized in several studies \citep{maNISTBulletSignature2004, vorburgerApplicationsCrosscorrelationFunctions2011, chuPilotStudyAutomated2010}. Additional features proposed for automatically assessing similarity also include signature distance \citep{maNISTBulletSignature2004}, striae depth and width \citep{ComputerIdentificationBullets1978}, and consecutive matching striae (CMS) \citep{chuAutomaticIdentificationBullet2013}.

\section{Methods}

In the matching algorithm proposed in \citet{aoas2}, several features are combined using a random forest \citep{breiman} to produce a similarity score based on 3D topographic scans of land engraved areas (LEAs). In order to generate these features, some pre-processing is necessary in order to transform the 3D surface measurements into `signatures' which can be compared.

\subsection{Automated Processing of 3D Scans}

From each land engraved area a signature is derived using the process described in detail in \citet{aoas2}:

\begin{itemize}
\item[(a)] Identify an area on the LEA with expressed striae. Locate a stable crosscut in the identified area; that is, an area where the striae are similar in the region above and below the crosscut.
\item[(b)] Discard extraneous/contaminated data, such as data from groove engraved area and areas affected by break-off or contact with objects after the bullet exited the barrel  (``tank rash").
\item[(c)] Remove bullet curvature using a non-parametric smooth. 
\end{itemize}
A signature for a land engraved area is then defined as the sequence $S(x)$, $x = 1, ..., I$, where $I$ is the number of observed locations across the base of the bullet. \autoref{fig:signatures} shows a set of six signatures corresponding to the six land engraved areas of bullets 1 and 2 from barrel 1 of set 44 of the Hamby study \citep{hamby}. 
<<signatures, fig.cap="Signatures of all six lands of bullets 1 and 2 from barrel 1 of the Hamby set 44. Matching lands between the two bullets have been placed above each other.", fig.height=4, fig.width=10>>=
# signatures for one bullet
# h44cc <- readRDS("data/h44-ccdata.rda")
# br1 <- h44cc[1:12]
# dframe <- 1:12 %>% purrr::map_df(.f = function(i) {
#   d <- br1[[i]]
#   d <- d %>% group_by(y) %>% summarize(
#     value = mean(value, na.rm=TRUE)
#   )
#   d$id <- i
#   d %>% ungroup(y) %>% rename(
#     x = y
#   )
# })
#
# dframe$bullet <- rep(c("Bullet 1", "Bullet 2"), each = 6)[dframe$id]
# dframe$land <- paste("Land", (dframe$id %% 6 + 1))
#
# dframe <- dframe %>% group_by(bullet, land) %>% nest()
# names(dframe)[3] <- "ccdata"
#
# library(bulletxtrctr)
# dframe <- dframe %>% mutate(
#   grooves = ccdata %>%
#     purrr::map(.f = cc_locate_grooves, method = "middle",
#                adjust = 30, return_plot = TRUE)
# )
# dframe <- dframe %>% mutate(
#   sigs = purrr::map2(
#     .x = ccdata, .y = grooves,
#     .f = function(x, y) {
#       cc_get_signature(
#         ccdata = x, grooves = y, span1 = 0.75, span2 = 0.03)
#     })
# )
# dframe$barrelland <- dframe$land
# idx <- which(dframe$bullet=="Bullet 2")
# dframe$landid <- parse_number(dframe$land)
# dframe$landid[idx] <- (dframe$landid[idx] +1) %% 6 + 1
# dframe$land <- paste("Land", dframe$landid)
# signatures <- dframe %>% select(bullet, land, sigs) %>% tidyr::unnest()

signatures <- read.csv("data/signatures.csv")
signatures %>%
  ggplot(aes(x = x / 1000)) +
#  geom_line(aes(y = raw_sig), colour = "grey70") +
  geom_line(aes(y = sig), colour = "grey30") +
  facet_grid(bullet ~ land) +
  ylim(c(-5, 5)) +
  theme_bw() +
  ylab("Surface measurement (in microns)") +
  xlab("Relative Location (in millimeters)")
@

Once signatures have been extracted from stable regions with expressed striae, they must be aligned in order to assess their similarity, just as examiners would manually align two bullets under a comparison microscope. Maximized cross correlation is used to pair-wise align signatures \citep{aoas2,vorburgerApplicationsCrosscorrelationFunctions2011}. 

The cross correlation (CC) function between two signatures $S_1(x)$ and $S_2(x)$ is defined as 
\[
CC(S_1, S_2, z) = \text{cor}\left(S_1(x), S_2(x + z)\right)
\] 
where $x$ and $x+z$ are integer values appropriately defined within the domains of $S_1$ and $S_2$, $z$ is the lag between the two sequences, and $\text{cor}(.,.)$ is the Pearson correlation coefficient.
When signatures contain missing values, pairwise complete observations are used to calculate the cross correlation. The lag $z$ used to achieve maximum $CC$ is used to determine the best alignment.


\subsection{Statistics for Matching Aligned Signatures}

Using the lag determined for aligning two signatures, other quantitative features describing the similarity of the two signatures can be extracted. Numerical features such as cross correlation and Euclidean distance
can be computed from the aligned signatures alone. Other features include characteristics of the signature that firearms and toolmark examiners base their visual assessment on, such as striae depth, number of matching striae, and the number of consecutively matching striae (CMS), one of the measures firearms examiners use to describe the similarity of two bullets quantitatively.

In order to evaluate CMS, we must first identify peaks and valleys in each of the signatures, then determine whether these peaks and valleys overlap sufficiently. Features which firearms examiners assess visually depend on the identification of these extrema. If there are at least six consecutive matching striae, the bullets are considered to be similar in practice \citep{biasotti_firearms_1997,nichols_scientific_2006}. CMS in this setting are counted as the number of consecutively matching peaks in signatures of two aligned lands. Peaks -- rather than peaks and valleys -- are used for this determination in part because it is difficult to visually assess valleys except as relative to peaks. Utilizing automatic alignment based on maximum cross correlation (as opposed to matching striation marks) allows the possibility that two lands will have zero consecutive matching striae; this would not occur with manual alignment based on striation marks, as the examiner typically lines up two distinctive markings, resulting in at least one matching striation mark. 

% <!-- I really think this belongs in this section: it's discussing cross correlation as a scoring statistic, rather than as a convenient way to align signatures so that scoring stats can be calculated. --> 

Cross correlation can also be used to assess similarity between two aligned signatures. Cross correlation varies between -1 and 1, but as the alignment is based on the maximized cross correlation value, in practice the cross correlations for aligned signatures are generally positive. Cross correlation values which are close to 1 indicate similarity between signatures, and may consequentially indicate that the two signatures come from the same source. Low cross correlation values indicate signatures which may originate from different sources.

The random forest presented in \citet{aoas2} is based on a combination of multiple characteristics, including cross correlation, number of matching striae, and number of consecutively matching striae. The output of the random forest is a score between 0 and 1 representing the algorithm's assessment of the similarity of the two signatures, where scores close to 1 indicate it to be highly likely that the aligned signatures come from the same source, while scores close to 0 indicate that the aligned signatures are highly likely to originate from different sources.

\subsection{Interpreting the Algorithm Score}

The features described above are each individually designed to separate same-source and different-source signatures: for instance, same-source signatures would have high cross correlation values and different-source signatures would have low cross correlation values. An automated algorithm selects a cutoff value, introducing a barrier between identification and exclusion. 

This threshold system introduces a binary classification: identification or exclusion. This definition of error is more stringent than the AFTE Theory of Identification, in that it does not allow for inconclusive results. This increased rigor may increase the error rate of the model when compared to examiner error rates, but we expect that the increased information available from the 3D measurement data will compensate for this loss. A binary decision model is also easier to interpret and provides more clear-cut, definitive results than the AFTE Theory of Identification system used in most forensic laboratories in the United States. Of course, in jurisdictions which utilize score-based likelihood ratios, this threshold system is not necessary because likelihood ratios are evaluated on a continuous spectrum (though, there is a natural threshold at 1, and many times a more flexible threshold is used to determine the level of significance of a result). Working under the binary decision model common in US jurisdictions, we can enumerate characteristics of an ideal similarity scoring mechanism: 

\begin{description}
\item[(R1)] {\bf Monotonicity:} a higher score is indicative of higher similarity between a pair of bullets, in particular, similarity scores of same-source pairs of bullets are higher than different-source pairs.
\item[(R2)] {\bf Stability:} the same score leads to the same conclusion in all situations and under separately assembled reference distributions. 
\end{description}

In particular, requirement (R2) would imply that the same threshold value would be used for all comparisons of a certain scoring mechanism. If the same-source and different-source similarity score distributions overlap, setting a threshold value will introduce classification errors.


\subsection{Identification Errors and Algorithm Evaluation}

When evaluating algorithm performance, it is useful to systematically assess  the set of possible outcomes. If ground truth and the algorithm's prediction match, we have either a correct identification or a correct exclusion. If ground truth and the algorithm's prediction do not match, we distinguish between two types of errors, which we will refer to as \textbf{wrong identification} and \textbf{missed identification}. Wrong identifications are those in which two bullets from different sources are determined to be from the same source. Missed identifications are those in which two bullets from the same source are determined to be from different sources. The full range of possible outcomes is shown in \autoref{tab:possible-outcomes}.

\begin{table}
\centering
\bgroup
\def\arraystretch{.8}%  1 is the default, change whatever you need
\begin{tabular}{|ccc}
\multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Ground Truth}}\\\cline{2-3}
\multicolumn{1}{c}{\textbf{Results}} & \multicolumn{1}{|c}{Same Source} & \multicolumn{1}{|c|}{Different Source} \\\hline
\multirow{2}{*}{Exclusion} & \multicolumn{1}{|c}{Missed} & \multicolumn{1}{|c|}{Correct}\\
   &  \multicolumn{1}{|c}{Identification} & \multicolumn{1}{|c|}{Exclusion}\\\hline
\multirow{2}{*}{Identification}  &  \multicolumn{1}{|c}{Correct} & \multicolumn{1}{|c|}{Wrong}\\
   &  \multicolumn{1}{|c}{Identification} & \multicolumn{1}{|c|}{Identification}\\\hline
\end{tabular}
\egroup
\caption{Possible outcomes of an examination of two pieces of evidence. Correct decisions are shown in the top-right and bottom-left corners; incorrect decisions in the top-left and bottom-right corners.\label{tab:possible-outcomes}}
\end{table}


In machine learning and statistics, it is common to evaluate algorithms using sensitivity and specificity; these concepts are related to the error rate. \textbf{Sensitivity} is defined as the proportion of actual positives that are correctly identified; that is, when ground truth is same source, the sensitivity is the proportion of correct identifications. \textbf{Specificity} is defined as the proportion of actual negatives that are correctly identified; that is, when ground truth is different source, the specificity is the proportion of correct identifications. The combination of sensitivity and specificity is sufficient to describe the reliability of an algorithm. 

When evaluating the \citet{aoas2} algorithm, we will describe errors as a percentage of correct evaluations. Statistics that do not translate to percent-form interpretation, such as area under the curve (AUC), will be presented as a decimal. 

\subsection{Assessing Bullet-to-Bullet Similarity}

Each of the scoring methods we have described is computed on a land-to-land basis. While these comparisons are useful, the question of interest typically involves the entire physical object (e.g. all lands on a single bullet), and a conclusion on same or different source should be reached based on the evidence of all lands of the bullets.

<<demo-l2lscores, echo=FALSE, results = 'hide', warning = FALSE, message = FALSE>>=
features <- read.csv("data/fsi-features.csv.gz")
features <- features %>% mutate(
  bullet1 = gsub("ullet ", "", bullet1),
  bullet2 = gsub("ullet ", "", bullet2)
)
b12 <- features %>% filter(
  group1 == "G1", barrel1 == "KA", bullet1 == "B1",
  group2 == "G1", barrel2 == "KA", bullet2 == "B2"
)
# 
# p1 <- b12 %>%
#   ggplot(aes(x = land1, y = land2, fill = rfscore)) +
#   geom_tile() + ggtitle("Same barrel") +
#   scale_fill_gradient2("RF score: ", midpoint = 0.3, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
#   theme_bw() +
#   xlab("Bullet 2") + ylab("Bullet 1") +
#   theme(legend.position = "bottom") + coord_equal()
# 
bab <- features %>% filter(
  group1 == "G1", barrel1 == "KA", bullet1 == "B1",
  group2 == "G1", barrel2 == "KB", bullet2 == "B1"
)
# p2 <- bab %>%
#   ggplot(aes(x = land1, y = land2, fill = rfscore)) +
#   geom_tile() + ggtitle("Different barrel") +
#   scale_fill_gradient2("RF score: ", midpoint = 0.3, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
#   theme_bw() +
#   xlab("Bullet 2") + ylab("Bullet 1") +
#   theme(legend.position = "bottom") + coord_equal()

l2lscores <- bind_rows(
  mutate(b12, type = "Same barrel"),
  mutate(bab, type = "Different barrel")
) %>% 
  mutate(type = factor(type, levels = c("Same barrel", "Different barrel"), ordered = T))



@

Land-to-land comparisons lead to a whole set of scores for bullet-to-bullet comparisons. \autoref{fig:b2b} shows two matrices of scores for two pairs of bullet-to-bullet matches. On the left, a matrix is shown that is typical for scores from two bullets from the same barrel. On the right, values for a pair of known non-matching bullets are shown.


<<b2b, echo = FALSE, fig.width = 5.4, fig.height = 3, out.width = ".6\\textwidth", fig.cap="Overview of land-land matching scores for two pairs of bullet-bullet comparisons. On the left the two bullets are known to come from the same source (barrel) on the right, the bullets are from two different sources (barrels).">>=
# grid.arrange(p1, p2, ncol = 2, padding = 2)
ggplot(data = l2lscores, aes(x = land1, y = land2, fill = rfscore)) +
  geom_tile() + 
  scale_fill_gradient2("Random Forest\n(RF) score: ", midpoint = 0.3, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  theme_bw() +
  xlab("Bullet 2") + ylab("Bullet 1") +
  theme(legend.position = "right") + coord_equal() + facet_wrap(~type)
@

While some imaging systems, such as BulletTrax 3D\footnote{https://www.ultra-forensictechnology.com/en/our-products/ballistic-identification/bullettrax/} or BalScan\footnote{https://www.forensic.cz/en/products/balscan}, may capture lower-resolution scans of an entire bullet at one time, confocal light microscopes do not have 360$^\circ$ capability. Thus, when imaging bullets using a confocal light microscope, operators scan one land at a time in a clockwise (left twisted rifling) or anti-clockwise (right twisted rifling) sequence. The order in which scans are acquired is kept as meta-information. 

Let us assume that lands on a bullet are labelled $\ell_i$ with $i = 1, .., p$, where $p$ indicates the number of lands a bullet has, as determined by the rifling of a barrel. For all of the bullets considered here the number of lands $p$ is 6. A match between  a pair of lands from two bullets therefore results in an expected additional $p-1$ matches between pairs of lands. These lands are also expected to be in a sequence, i.e. if there is a match between lands $\ell_i$ on bullet 1 and $\ell_j$ on bullet 2, we also expect lands $\ell_{i\oplus s}$ and $\ell_{j\oplus s}$ to match for all integers $s$, where $\oplus$ is defined as $a \oplus b \equiv \left((a + b - 1)\mod p\right) + 1$. This relationship gives rise to the sequence average maximum (SAM) to quantify a bullet-to-bullet match. 

\begin{definition}{Sequence Average and its Maximum}
Let $A$ be a square real-valued matrix of dimensions $p \times p$. For the purpose of this paper, $A$ consists of scores describing the similarity between two sets of land engraved areas. The entries in $A$ are represented as $a_{i,j}$, where $i$ and $j$ are row and column indexes. 

The $k$th \emph{sequence average} $SA(A, k)$ for $k = 0, ... p-1$ is defined  as 
$$SA(A, k) = \frac{1}{p} \sum_{i=1}^{p} a_{i,i \oplus k}, \text{ where } i\oplus k := \left((i + k - 1)\mod p\right) + 1.$$
The \emph{Sequence Average Maximum} \citep[SAM, ][]{sam} of square matrix $A$ of scores is defined as
$$SAM (A) = \max_{k = 1}^{p} SA(A, k).$$
\end{definition}
Looking back at \autoref{fig:b2b}, we see that for the two bullets from the same barrel, the sequence average for 
$k=2$ is higher than the other sequence averages, and also higher than the sequence averages for the other pair of bullets shown on the right of the figure. SAM scores allow us to define a single quantity for each pair of bullets that describes the similarity between these two bullets. 

The sequence average maximum of the correlation between lands is used in SensoComp \citep{sensofar-sam} to capture the similarity between bullets. The correlation based SAM score has also been called the 'average correlation calculated at the max phase' in \citet{chuPilotStudyAutomated2010}. 

<<sam-sketch, out.width='\\textwidth', fig.width=8, fig.height = 3, fig.cap="Sketch of all six land-to-land sequences between two bullets with six lands.">>=
df2 <- data.frame(expand.grid(x = 1:6, y = 1:6))
df2$case1 <- df2$x == ((df2$y + 5) %% 6) + 1
df2$case2 <- df2$x == ((df2$y + 0) %% 6) + 1
df2$case3 <- df2$x == ((df2$y + 1) %% 6) + 1
df2$case4 <- df2$x == ((df2$y + 2) %% 6) + 1
df2$case5 <- df2$x == ((df2$y + 3) %% 6) + 1
df2$case6 <- df2$x == ((df2$y + 4) %% 6) + 1

df2 %>%
  gather("case", "values", starts_with("case")) %>%
  mutate(case = gsub("case", "", case)) %>%
  mutate(case = paste0("k = ", as.numeric(case) - 1)) %>%
  ggplot(aes(x = factor(x), y = factor(y, levels = 1:6), fill = values)) +
  geom_tile(colour = "grey20") +
  facet_wrap(~case, ncol = 6) +
  xlab("Bullet 2") + ylab("Bullet 1") +
  scale_x_discrete(labels = paste0("L", 1:6)) +
  scale_y_discrete(labels = paste0("L", 1:6)) +
  scale_fill_manual("Cell included in SA(A,k)", values = c("white", "grey20")) +
  coord_equal() +
  theme(legend.position = "bottom")
@


In this paper, we validate the algorithm described in \citet{aoas2} on three external test sets. We assess the results on each set and evaluate whether the minimal requirements described above are fulfilled for bullet-to-bullet SAM scores. We compare the random forest algorithm's performance to the performance of other suggested measures for quantitative assessment of bullet similarity, such as cross correlation and consecutive matching striae. 

\section{Validation Sets}

Random forests \citep{breiman} have a built-in internal testing mechanism to prevent potential overfitting. Errors reported by random forest algorithms are based on these internal test sets. However, there is some discussion on whether or in which situations these errors are biased \citep{rfOver2, rfOver}. 
Neither of these papers addresses another issue with internal test sets: internal test sets are constructed to have the same distribution as the training data (apart from sampling variability). For any machine learning method, a true benchmark of the performance of an algorithm requires testing its performance on external test data. Good performance (in terms of wrong and missed identifications) on external test data validates a ML algorithm. External data also allows an assessment of the algorithm's sensitivity to distributional changes as well as testing the algorithm's robustness by going outside the parameters of the training data.  

The algorithm in \citet{aoas2} is trained on scans from Hamby sets 252 and 173 made available through \citet{nist}. Set 173 was originally published as Hamby set 44. This mis-labeling has been corrected now. The scans used to train the model were taken at 20 fold magnification for a resolution of 1.5625 microns per pixel. While magnification is generally of interest in microscopy, \emph{resolution} -- usually measured in microns per pixel -- is of more interest for 3D topographic measurements, as it determines the operative level of available data.


For the validation of the automatic matching algorithm we are considering three validation sets of -- what should be -- increasing difficulty level: 
\begin{itemize}
\item {\bf Hamby set 44} is one set of the Hamby study \citep{hamby}. Each Hamby set  consists of a total of 35 bullets fired through ten consecutively manufactured barrels of Rugers P85. Each set consists of 20 known bullets (two from each of the ten barrels) and 15 questioned bullets of unknown origin. Note that all Hamby sets are closed sets; that is, all questioned bullets are fired through one of the ten barrels. The ammunition used for this set were 9 mm Luger 115 Grain Full Metal Jacket  from the Winchester Ammunition Company.
\item {\bf Phoenix PD}
Tylor Klep from Phoenix PD provided sets of known test fires and questioned bullets: the set of known bullets consists of three test fires (B1, B2, B3) from each of eight different, consecutively rifled Ruger P-95 barrels (A9, C8, F6, L5, M2, P7, R3, U10). Ten questioned bullets were provided (B, E, H, J, K N, Q, T, Y, Z). This set is an open set; that is, it is not known in advance whether all (or any) of the questioned bullets are fired from the known barrels. In fact, the results will show that three of the questioned bullets were fired from three different barrels not included in the knowns. These three barrels correspond to an eleventh Ruger P-95 barrel, a Ruger P-95C barrel and a Ruger P-85 barrel. All bullets fired for this study are American Eagle 9mm  Luger full metal jackets.
Land engraved areas for each of the six lands of each bullet were scanned by Bill Henderson (Sensofar).
\item {\bf Houston FSI}
This study was set up by Melissa Nally and Kasi Kirksey from FSI Houston. Three test sets based on ten consecutively rifled Ruger LCP barrels (A, B, C, D, E, F, G, H, I, J) and three other, non-consecutively rifled Ruger LCP barrels (R1, R2, R3). Each test set consists of three test fires each from five consecutively rifled barrels. Additionally, ten questioned bullets are provided for each kit. The ammunition used in both test fires and the questioned bullets were Remington UMC 9mm Luger Full Metal Jackets. All three of the test sets are open; that is, not every one of the questioned bullets is fired from the five known barrels in each of the test set.
The structure of these three sets is similar to a forthcoming study by Nally and Kirksey, but the results here come from preliminary test sets made available to us.
\end{itemize}
 
Scans of all land engraved areas for the validation data were taken on a Sensofar Confocal Light microscope at 20x magnification resulting in a resolution of 0.645 microns per pixel. If not indicated otherwise, scans were taken at the Roy J Carver high resolution microscopy lab at Iowa State University.
 
Case studies were chosen such that difficulty for the matching algorithm increases with each case study: Hamby set 44 is part of the Hamby study. The algorithm in \citet{aoas2} was trained on sets 173 and 252, so the bullets in Hamby 44 are of the same type of ammunition and are fired through the same barrels as the bullets in the training set. The Phoenix PD set uses Ruger P-95 barrels, which are different from the barrels in the Hamby sets used for training the algorithm, but the barrels are rifled similarly to the Ruger P-85 barrels. The Houston sets use Ruger LCP barrels. These barrels are first rifled traditionally, i.e. similar to the Ruger P-85, but are treated to a secondary round of heating after rifling. This second round of heat exposure has the potential to introduce some subclass characteristics, i.e.\ anomalies that are shared between different barrels manufactured in this manner. This should make the automatic matching harder, and in particular, is expected to complicate the classification as different-source land engraved areas/bullets. In all three studies, the scan resolution is much higher than the scans used to train the algorithm in \citet{aoas2}; this difference also provides a test of the algorithm's ability to generalize to scans taken at different resolutions.

\section{Results}

\subsection{Hamby Set 44}

<<h44, echo=FALSE, fig.width = 8, fig.height=4, fig.cap="Hamby set 44: overview of all bullet-to-bullet matches between all pairs of questioned bullets (y-axis) and known test fires from ten barrels (x axis).">>=
h44features <- read.csv("data/h44-features.csv")
h44features <- h44features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)
# tank rash lands:
#h44features <- h44features %>% mutate(
#  rfscore = replace(rfscore, (bullet1 == "I" & land1 == 6) | (bullet2 == #"I" & land2 == 6), NA)
#)
h44features <- h44features %>% mutate(
  rfscore = replace(rfscore, (barrel1 == 8 & bullet1 == "1" & land1 == 6) | (barrel2 == 8 & bullet2 == "1" & land2 == 6), NA)
)

h44nest <- h44features %>%
  filter(barrel1 == "Unk") %>%
  filter(barrel1 != barrel2 | bullet1 != bullet2) %>%
  mutate(
    barrel1 = "Unknown",
    barrel2 = as.character(barrel2),
    barrel2 = replace(barrel2, barrel2 == "Unk", "Unknown")
  ) %>%
  group_by(bullet1, barrel2, bullet2) %>%
  nest()

h44nest <- h44nest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)
h44nest <- h44nest %>%
  mutate(
    barrel2 = factor(barrel2, levels = c(1:10, "Unknown")),
    bullet1 = factor(bullet1, levels = rev(c(
      "K", "O", "L", "P",
      "J",
      "H", "I", "Y", "G",
      "E", "U", "X", "F",
      "T", "S"
    ))),
    bullet2 = factor(bullet2, levels = c(1:2, rev(levels(bullet1))))
  )
h44 <- h44nest %>%
  ggplot(aes(
    y = bullet1, x = bullet2,
    fill = sam_rf
  )) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, space = "free", scales = "free") +
  ylab("Questioned bullets") +
  xlab("Test fires from barrels 1 to 10 (left), questioned bullets (right)") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.35, limits = c(0, 1)
  ) +
  scale_colour_manual("same source", values = "darkorange") +

  #  scale_x_discrete(position = "top") +
  scale_y_discrete() +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(h44nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Hamby set 44") +
  theme_bw() +
  theme(legend.position = "bottom")


h44nest$samesource <- h44nest$sam_rf > 0.3

h44
@

\autoref{fig:h44} shows an overview of all scores from pairs of questioned bullets with all other bullets. On the left of \autoref{fig:h44} there are ten strips labelled 1 through 10. These strips correspond to known barrels 1 through 10. Each barrel was test-fired twice, so each of the questioned bullets (shown along the $y$ axis) matches two bullets fired from a known barrel. Colored tiles are used to show the strength of the comparison; light grey colors correspond to low similarity scores, dark colors correspond to high similarity scores. Ground truth is encoded in this figure as a thin, dark, colored frame for all pairs of same-source bullets. 
Ideally, we want to see one barrel with two dark-filled, dark-framed tiles for each questioned bullet, and light grey tiles for all other barrels, indicating a match between a questioned bullet and a single barrel. This expectation is met for all questioned bullets, i.e.\ the automatic matching  identifies the correct barrel for all questioned bullets. For two of the questioned bullets, 'I' and 'F', the similarity scores to the matching barrels are considerably smaller than for the other questioned bullets. 

The right side of \autoref{fig:h44} shows the relationship between all pairs of questioned bullets. Note that questioned bullets are not compared to themselves, leaving white squares on the diagonal. Some of the questioned bullets match the same barrel, e.g.\ questioned bullets 'P' and 'J' both match barrel 5. Therefore bullets 'P' and 'J' also match each other in the square on the right hand side.

<<close, eval=FALSE, fig.width=11.25, fig.height=4.2, fig.cap="Examples of land-to-land comparisons from the Hamby-44 set. Along the top are the three bullets from different source with the highest RF scores, along the bottom are the three bullets from same source with the lowest RF score.", fig.subcap=c("Ground truth: different source", "Ground truth: same-source"), fig.align="middle", fig.ncol=1>>=
# ex2 <- h44nest %>% filter(samesource == FALSE & sam_rf >= 0.287)
# rfs2 <- ex2$sam_rf
# ex2 <- ex2 %>% mutate(
#   data = data %>% purrr::map(.f = function(d) {
#  #   browser()
#     index <- which.max(bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore))
#     d$same <- ((d$land2 - d$land1) %% 6)  == index-1
#
#   #   d %>% ggplot(aes(x = land1, y=land2, fill=rfscore)) +
#   #                  geom_tile() +
#   #     geom_tile(colour="darkorange", data = d %>% filter(same), size=0.5) +
#   #     scale_fill_gradient2(low="darkgrey", high = "darkorange",
#   #                      midpoint = 0.5) +
#   #     scale_colour_manual(values=c( "darkorange")) +
#   # scale_x_discrete("") +
#   # scale_y_discrete("")
#     d
#   })
# )
# ex2$example <- with(ex2, paste0(bullet1, " vs ", barrel2, "-", bullet2))
# ex2 <- unnest(ex2, data)
# ex2 %>%
#   ggplot(aes(x = land1, y = land2, fill=rfscore)) +
#   geom_tile() +
#   theme_bw() +
#   scale_fill_gradient2("RF score", low="darkgrey", high = "darkorange",
#                        midpoint = 0.5) +
# #  geom_tile(colour="darkgrey", data = ex2 %>% filter(same), size=0.5) +
#   facet_grid(.~example) +
#   scale_x_discrete("") +
#   scale_y_discrete("") +
#   annotate(geom="text", x = 0.5, y = 6.75, label=sprintf("SAM RF: %.3f", rfs2), hjust = 0)
#
#
#
# ex1 <- h44nest %>% filter(samesource == TRUE & sam_rf < 0.4)
# rfs <- ex1$sam_rf
# ex1 <- ex1 %>% mutate(
#   data = data %>% purrr::map(.f = function(d) {
#  #   browser()
#     index <- which.max(bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore))
#     d$same <- ((d$land2 - d$land1) %% 6)  == index-1
#
#   #   d %>% ggplot(aes(x = land1, y=land2, fill=rfscore)) +
#   #                  geom_tile() +
#   #     geom_tile(colour="darkorange", data = d %>% filter(same), size=0.5) +
#   #     scale_fill_gradient2(low="darkgrey", high = "darkorange",
#   #                      midpoint = 0.5) +
#   #     scale_colour_manual(values=c( "darkorange")) +
#   # scale_x_discrete("") +
#   # scale_y_discrete("")
#     d
#   })
# )
# ex1$example <- with(ex1, paste0(bullet1, " vs ", barrel2, "-", bullet2))
# ex1 <- unnest(ex1, data)
# ex1 %>%
#   ggplot(aes(x = land1, y = land2, fill=rfscore)) +
#   geom_tile() +
#   theme_bw() +
#   scale_fill_gradient2("RF score", low="darkgrey", high = "darkorange",
#                        midpoint = 0.5, na.value = NA) +
#   geom_tile(colour="darkorange", data = ex1 %>% filter(same), size=0.5) +
#   facet_grid(.~example) +
#   scale_x_discrete("") +
#   scale_y_discrete("") +
#   annotate(geom="text", x = 0.5, y = 6.75, label=sprintf("SAM RF: %.3f", rfs), hjust = 0)


# \autoref{fig:close} shows an example of six comparisons of bullet pairs at the land-to-land level. The panels along the top of the figure show three bullet pairs that are known to be from different barrels, the three bullet pairs along the bottom are known to be from the same barrel. The light orange frame in the bottom three panels indicate which lands correspond to one-another in the matching. It is obvious that not all of the scores are as high as we would usually see in same-source comparisons.
@

\subsection{Phoenix PD}

<<pd, echo=FALSE, fig.width = 8, fig.height=3.56, fig.cap="Phoenix study: overview of all bullet-to-bullet matches between all pairs of questioned bullets (y-axis) and known test fires from ten barrels (x axis). The order of the bullets on the y axis is determined by matching barrel.">>=
features <- read.csv("data/pd-features.csv.gz")
features <- features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)
# change labels for land 3 and land 4 of bullet B3 in P7
features <- features %>% mutate(
  first = as.character(first),
  first = replace(first, first=="Gun 1-P7/B3/L4.dat", "XXX"),
  first = replace(first, first=="Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"),
  first = replace(first, first=="XXX", "Gun 1-P7/B3/L3.dat")
)

features <- features %>% mutate(
  KM = replace(KM, first %in% c("Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"), FALSE)
)
features <- features %>% mutate(
  KM = ifelse(first == "Gun 1-P7/B3/L3.dat", (second == "Unknown 1-J/L3.dat"), KM),
  KM = ifelse(first == "Gun 1-P7/B3/L4.dat", (second == "Unknown 1-J/L4.dat"), KM)
)



f2 <- features %>%
  separate(first, into = c("foo1", "foo2", "barrel1", "bullet1", "land1", "foo3"), remove = FALSE) %>%
  separate(second, into = c("foo4", "foo5", "bullet2", "land2", "foo6"), remove = FALSE) %>%
  select(-foo1, -foo2, -foo3, -foo4, -foo5, -foo6)


unknowns <- read.csv("data/pd-features-unknown.csv")
unknowns <- unknowns %>% filter(bullet1 != bullet2)
unknowns <- unknowns %>% mutate(
  bullet2 = gsub("Unknown 1-", "", bullet2),
  bullet1 = gsub("Unknown 1-", "", bullet1)
)
f3 <- rbind(
  f2 %>%
    select(
      b1, b2, barrel1, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, KM
    ) %>%
    mutate(barrel2 = "Unknown"),
  unknowns %>%
    select(
      b1, b2, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, 
    ) %>%
    mutate(barrel1 = "Unknown", barrel2 = "Unknown", KM = FALSE)
)

pdnest <- f3 %>% group_by(bullet2, barrel1, bullet1) %>% nest()

pdnest <- pdnest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  KM = data %>% purrr::map_lgl(.f = function(d) {
    any(d$KM)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)

pdnest <- pdnest %>%
  mutate(
    bullet2 = factor(bullet2, levels = rev(c("N", "B", "E", "T", "H", "J", "K", "Q", "Y", "Z"))),
    bullet1 = factor(bullet1, levels = c("B1", "B2", "B3", rev(levels(bullet2))))
  )
p3 <- pdnest %>%
  ggplot(aes(y = bullet2, x = bullet1, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel1, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.45, limits = c(0, 1)
  ) +
  scale_colour_manual("same source", values = "darkorange") +
  scale_y_discrete() +
  theme_bw() +
  theme(legend.position = "bottom") +
  #  coord_equal() +
  geom_tile(aes(colour = TRUE),
    size = .5,
    data = filter(pdnest, KM)
  ) +
  guides(colour = guide_legend(override.aes = list(fill = NA)))


pdnest$samesource <- pdnest$KM

p3 + ggtitle("Phoenix PD set")
@

\autoref{fig:pd} shows an overview of similarity scores for all pairs of questioned bullets and  test fires. The color encoding is the same as in the previous figures. Here, we see that questioned bullets either match all 3 bullets fired out of the same barrel for exactly one of the barrels or none of the known barrels. Questioned bullets 'Q', 'Y', and 'Z', do not match any of the known barrels.
None of the test fires from barrel U10 match any of the questioned bullets.  This is a sign of the previously mentioned open set characteristic of the study. 
Once again, the results from automatic matching correctly pair questioned bullets with their corresponding barrels.

\subsection{Houston FSI}

\autoref{fig:hou} shows an overview of the three sets of scores for all pairs of questioned bullets with all other bullets in the set. In set 1, five of the questioned bullets can be matched to known bullets in the set. None of the questioned bullets in set 1 match bullets fired from barrels KC or KD. Additionally, two of the questioned bullets which do not match any of the barrels in the set match each other (U28 and U37). In set 2, four of the questioned bullets can be matched to known bullets in the set; of the additional four questioned bullets which do not match any known bullets, two were fired from the same barrel (bullets U34 and U73). In set 3, six of the questioned bullets match known bullets, and the remaining two questioned bullets match each other (bullets U14 and U45) but do not match any known bullets in the set. 
% <!--Thus, we can conclude that the algorithm performs well on open set studies, as it can identify matches among questioned bullets which do not match any known bullets. -->
While all bullet-to-bullet matches are correctly identified, and no known matches are missed, it is obvious from the generally lighter shades of the tiles corresponding to matching bullets in \autoref{fig:hou}(b) that the algorithm is not performing as well on set 2 as it performs on sets 1 and 3.

<<hou, fig.width=8, fig.height = 2.67, fig.cap="Overview of matching scores for all pairs of questioned bullets to known bullets from five barrels (on the left) and questioned bullets to themselves (tiles on the right).", fig.subcap=c("Set 1", "Set 2", "Set 3"), fig.align="middle", fig.ncol=1>>=
features <- read.csv("data/fsi-features.csv.gz")
features <- features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)

features <- features %>% mutate(
  bullet1 = gsub("ullet ", "", bullet1),
  bullet2 = gsub("ullet ", "", bullet2)
)

features <- features %>%
  filter(group1 == group2) %>%
  filter(barrel1 == "Unknowns") %>%
  filter(barrel1 != barrel2 | bullet1 != bullet2) 

groups <- features %>%
  group_by(group1, barrel1, bullet1, barrel2, bullet2) %>%
  nest()


hounest <- groups %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)

hounest$samesource <- hounest$sam_rf > 0.3

hou1nest <- hounest %>%
  filter(group1 == "G1") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U40", "U36", "U42", "U10", "U77", "U37", "U28", "U15"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )

hou1 <- hou1nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou1nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 1")


hou2nest <- hounest %>%
  filter(group1 == "G2") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U23", "U41", "U61", "U98", "U73", "U34", "U63", "U66"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )
hou2 <- hou2nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou2nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 2")

hou3nest <- hounest %>%
  filter(group1 == "G3") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U27", "U33", "U36", "U49", "U56", "U65", "U14", "U45"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )

hou3 <- hou3nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou3nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 3")

hou1

hou2

hou3
@


\section{Evaluating the Random Forest Algorithm}
The random forest SAM scores correctly separate known matches from known non-matches, using SAM scores to aggregate land-to-land scores into a bullet-to-bullet comparison. With no errors at the bullet-to-bullet level, we can now evaluate the scores in light of (R1) and (R2). In this section, we will compare the random forest scores to cross correlation scores; cross correlation is one of the components of the random forest, but has also been used to quantify the strength of a match in its own right \citep{chuPilotStudyAutomated2010}. In addition, both scoring methods use the same scale (0 to 1) and are continuous along that interval; other evaluation methods, such as consecutive matching striae, are discrete and more difficult to directly compare to continuous measurements.

\subsection{Comparison of SAM scores}
\autoref{fig:compare} shows density curves for the scores bullet-to-bullet matches of each of the studies, comparing the SAM scores based on cross correlation (top) and the random forest score (bottom). Color indicates ground truth. Small vertical lines below the $x$-axis indicate observed scores in a **rug-plot**. The plot shows that for all three case studies and both continuous measures, i.e.\ SAM scores based on cross correlation as well as SAM scores based on the random forest scores, all similarity scores are higher for pairs of bullets from the same source, i.e.\ bullets fired through the same barrel, than pairs of bullets from different sources (fired through different barrels). 
 
<<compare, fig.width = 8, fig.height = 7, fig.cap="Density curves of similarity scores from random forest scores (top), cross correlation (middle), and consecutive matching striae (bottom). Different colors indicate same source versus different source. Ideally all scores of different source pairs should be much lower than scores for same source pairs.">>=
nests <- rbind(
  h44nest %>%
    select(sam_rf, sam_ccf, sam_cms, samesource) %>%
    mutate(study = "Hamby 44", group1 = NA),
  pdnest %>%
    select(sam_rf, sam_ccf, sam_cms, samesource) %>%
    mutate(study = "Phoenix PD", group1 = NA),
  hounest %>%
    select(sam_rf, sam_ccf, sam_cms, samesource, group1) %>%
    mutate(study = "Houston FSI")
)
nests$study <- factor(nests$study, levels = c("Hamby 44", "Phoenix PD", "Houston FSI"))
nestslong <- nests %>% gather(key = "measure", value = "value", sam_rf, sam_ccf, sam_cms)
nestslong$measure <- factor(nestslong$measure)
levels(nestslong$measure) <- c("Cross correlation", "CMS", "RF score")

labels <- h44nest %>% filter(samesource, sam_rf < 0.5)
labels$comparison <- with(labels, paste(bullet1, "vs", paste(barrel2, bullet2, sep = "-")))
labels$measure <- "RF score"
labels$study <- "Hamby 44"
labels$y <- 5 + 4:1
p1 <- nestslong %>%
  filter(measure %in% c("RF score")) %>%
  ggplot(aes(
    x = value, fill = samesource,
    colour = samesource
  )) +
  geom_density(alpha = 0.6) +
  geom_rug(alpha = 0.3, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_x_continuous("SAM scores", limits=c(0,1)) +
  theme(legend.position = "bottom") +
  geom_hline(yintercept = 0, show.legend = F) +
  geom_text(aes(x = sam_rf, y = y, label = comparison),
            show.legend = F,
            hjust = 0,
            data = labels
  ) +
  geom_segment(aes(x = sam_rf, xend = sam_rf, y = 0, yend = y),
               show.legend = F,
               size = 0.1,
               colour = "grey20",
               data = labels
  ) + 
  theme(axis.title.x = element_blank(), axis.text.y = element_text(hjust = .5, angle = 90)) 

p2 <- nestslong %>%
  filter(measure %in% c("Cross correlation")) %>% 
  ggplot(aes(
    x = value, fill = samesource,
    colour = samesource
  )) +
  geom_density(alpha = 0.6) +
  geom_rug(alpha = 0.3, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_x_continuous("SAM scores") + 
  theme(axis.title.x = element_blank(), axis.text.y = element_text(hjust = .5, angle = 90)) 

p3 <- nestslong %>%
  filter(measure %in% c("CMS")) %>% 
  ggplot(aes(
    x = value, fill = samesource,
    colour = samesource
  )) +
  geom_density(alpha = 0.6) +
  geom_rug(aes(x = jitter(value)), alpha = 0.3, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_x_continuous("SAM scores")  + 
  theme(axis.text.y = element_text(hjust = .5, angle = 90)) 

grid.arrange(p1, p2, p3, nrow = 3, heights = c(1, 1, 1.4))
@

According to \autoref{fig:compare}, neither cross correlation nor random forest score fulfill both of the minimal requirements laid out in the introduction. Both measures fulfill (R1), i.e.\ all  scores  of  different-source pairs are lower than scores of same-source pairs for SAM scores based on cross correlation and RF scores.
However, SAM scores based on the multivariate random forest score show better separation between same-source and different-source scores than SAM scores based on cross correlation. This can be seen from the larger horizontal distance between the modes (peaks) of the density curves of RF based SAM scores compared to the cross correlation SAM scores.  

Regarding requirement (R2), \autoref{fig:compare} shows that a single cutoff value does not exist that separates same-source scores from different-source scores across all three studies without introducing errors.

There are several approaches for potential improvements: we can try to fine tune the matching algorithm to take make and model of the firearm and ammunition used into account or expand the training base of the algorithm to include a wider variety of makes and models. The downside of either of these options is that we would need to considerably expand the database used for training. Another solution would be to augment the aggregation used to get from land-to-land scores to bullet scores: SAM scores only take the scores of the maximum sequence into account and ignore scores associated with pairs of land engraved areas from different sources. Those scores might be useful in establishing a baseline that better expresses similarity between pairs of bullets. One approach that uses scores for both same source pairs and different source pairs are score-based likelihood ratios \citep{Bunch:2013if, Morrison:2018fh}.

\subsection{Digging deeper: comparison of land-to-land scores}

Examining land-to-land scores provides more details about the matching algorithm's performance. \autoref{fig:compare-land-to-land} shows density curves for each study. There is some overlap between known matching and known non-matching land-to-land scores; in many cases the random forest scores for these overlapping values are more extreme than the corresponding cross correlation scores. This suggests that the matching algorithm is sensitive to the presence of LEAs which have low land-to-land similarity scores, such as the matches of bullets 'I' and 'F' in the Hamby 44 study. Comparing this to the bullet-to-bullet score density plot shown in \autoref{fig:compare}, we see that there is much greater separation between the same-source and different-source densities for each study in the bullet-to-bullet comparisons than in the land-to-land comparisons, that is, one or two poorly matching lands does not prevent the bullet from showing a matching score, though if there are weak matches on several lands, such as the marked Hamby 44 comparisons in \autoref{fig:compare}, the overall score may be affected. 

Several lands in Hamby set 44 have major deficiencies,  such as 'tank rash' (a collision of the bullet after exiting the barrel with a surface causing markings on top of the striations from the barrel) or extreme pitting (holes caused by direct contact with burning gun powder).
\autoref{fig:tankrash} gives an overview of all lands of affected bullets in the Hamby-44 set. These issues affect the algorithm's ability to identify a stable signature, which impacts the subsequent similarity scores  at the land-to-land and bullet-to-bullet level.
<<tankrash, fig.cap="Overview of bullet lands with prominent deficiency such as tank rash or extreme pitting in the Hamby-44 study.", fig.width=4, fig.height = 2, out.width='47.5%', fig.subcap=c("Br1-B1-L6 (tank rask)", "Br2-B2-L5 (tank rash)", "Br3-B1-L5 (tank rash)", "Br8-B1-L6 (tank rash)", "Br8-B2-L2 (pitting)", "Br8-B2-L6 (tank rash)", "Unk-E-L6 (tank rash)", "Unk-I-L6 (tank rash and break-off)"), fig.align="middle", fig.ncol=2>>=
knitr::include_graphics(
  c(
    "./images/B1-B1-L6.png", "./images/B2-B2-L5.png",
    "./images/B3-B1-L5.png", "./images/B8-B1-L6.png",
    "./images/B8-B2-L2.png", "./images/B8-B2-L6.png",
    "./images/NA-BE-L6.png", "./images/NA-BI-L6.png"
  )
)
@

Three lands from bullets known to be fired from barrel 8 are affected, producing low scores for barrel 8 in \autoref{fig:h44}. We also see that questioned bullet 'I' is affected, explaining some of the low similarity scores for this bullet.
None of the lands of any of the bullets in the other studies are affected by tank rash in a similar manner.

<<cache=FALSE>>=
hounest <- hounest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0.05)
      d
    })
)
houlands <- hounest %>% unnest(data)

set.seed(20140501)
h44nest$samesource <- h44nest$sam_rf > 0.3
h44nest <- h44nest %>% 
  mutate(
    data = purrr::map2(.x = data, .y = samesource, .f = function(d, y) {
      d$samesource <- y
      d
  }
  ))

h44nest <- h44nest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      if (any(d$samesource)) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0.01)
      } else d$sameland <- FALSE
      d <- d %>% select(-samesource)
      d
    })
)
h44lands <- h44nest %>% unnest(data)

pdnest <- pdnest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0)
      d
    })
)
pdlands <- pdnest %>%
  rename(bullet_KM = KM) %>% 
  unnest(data)


all_lands <- rbind(
  h44lands %>%
    select(rfscore, ccf, cms, sameland, samesource) %>%
    mutate(study = "Hamby 44", group1 = ""),
  pdlands %>%
    select(rfscore, ccf, cms, sameland, samesource) %>%
    mutate(study = "Phoenix PD", group1 = ""),
  houlands %>%
    select(rfscore, ccf, cms, sameland, samesource, group1) %>%
    mutate(study = "Houston FSI")
)
all_lands$study <- factor(all_lands$study, levels = c("Hamby 44", "Phoenix PD", "Houston FSI"))
all_lands <- all_lands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
@

<<check44, eval=FALSE>>=
h44lands <- h44lands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
h44lands %>% 
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = h44lands %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)
  
@

<<checkpd, eval=FALSE>>=
pdlands <- pdlands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
pdlands %>% 
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = pdlands %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)
  
@

<<checkhou, eval=FALSE>>=
houlands <- houlands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
hou1 <- houlands %>% filter(group1==group2, group1=="G1") 

hou1 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou1 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)

hou2 <- houlands %>% filter(group1==group2, group1=="G2") 

hou2 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou2 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)  

hou3 <- houlands %>% filter(group1==group2, group1=="G3") 

hou3 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou3 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)  
@


<<compare-land-to-land, fig.width = 8, fig.height = 6.5, fig.cap="Density curves of land-to-land similarity scores from RF scores (top), cross-correlation (middle), and consecutive matching striae (CMS)(bottom). Different colors indicate same source versus different source for each land. RF scores for different source comparisons are generally well below 0.5. Some same-source comparisons are also below 0.5 for RF scores, indicating potential problems with at least one of the lands involved in the comparison.">>=
# all_lands %>% 
#   gather(key = "measure", value = "value", ccf, rfscore) %>%
#   mutate(measure = factor(measure, levels = c("ccf", "rfscore"),
#                           labels = c("Cross correlation", "RF score")),
#          study = paste(study, group1) %>% str_trim() %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))) %>%
#   ggplot(aes(
#     x = value, fill = sameland,
#     colour = sameland
#   )) +
#   geom_density(alpha = 0.6) +
# #  geom_rug(alpha = 0.5/12, show.legend = F) +
#   facet_grid(measure ~ study) +
#   theme_bw() +
#   scale_fill_manual("Same source", values = c("darkgrey", "darkorange")) +
#   scale_colour_manual("Same source", values = c("darkgrey", "darkorange")) +
#   scale_x_continuous("Land-to-Land scores", limits = c(0, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) +
#   theme(legend.position = "bottom") +
#   geom_hline(yintercept = 0, show.legend = F)
p1 <- all_lands %>% 
  gather(key = "measure", value = "value", ccf, cms, rfscore) %>%
  mutate(measure = factor(measure, levels = c("ccf", "cms", "rfscore"),
                          labels = c("Cross correlation", "CMS", "RF score")),
         study = paste(study, group1) %>% str_trim() %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))) %>%
  filter(measure == "RF score") %>%
  ggplot(aes(
    x = value, fill = sameland,
    colour = sameland
  )) +
  geom_density(alpha = 0.6) +
#  geom_rug(alpha = 0.5/12, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_x_continuous("Land-to-Land scores", limits = c(0, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) +
  theme(legend.position = "bottom", axis.title.x = element_blank(), axis.text.y = element_text(hjust = .5, angle = 90)) +
  geom_hline(yintercept = 0, show.legend = F)

p2 <- all_lands %>% 
  gather(key = "measure", value = "value", ccf, cms, rfscore) %>%
  mutate(measure = factor(measure, levels = c("ccf", "cms", "rfscore"),
                          labels = c("Cross correlation", "CMS", "RF score")),
         study = paste(study, group1) %>% str_trim() %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))) %>%
  filter(measure == "Cross correlation") %>%
  ggplot(aes(
    x = value, fill = sameland,
    colour = sameland
  )) +
  geom_density(alpha = 0.6) +
#  geom_rug(alpha = 0.5/12, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange"), guide = F) +
  scale_x_continuous("Land-to-Land scores", limits = c(0, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) +
  theme(legend.position = "bottom", axis.title.x = element_blank(), axis.text.y = element_text(hjust = .5, angle = 90)) +
  geom_hline(yintercept = 0, show.legend = F)

p3 <- all_lands %>% 
  gather(key = "measure", value = "value", ccf, cms, rfscore) %>%
  mutate(measure = factor(measure, levels = c("ccf", "cms", "rfscore"),
                          labels = c("Cross correlation", "CMS", "RF score")),
         study = paste(study, group1) %>% str_trim() %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))) %>%
  filter(measure == "CMS") %>%
  ggplot(aes(
    x = value, fill = sameland,
    colour = sameland
  )) +
  geom_density(alpha = 0.6) +
#  geom_rug(alpha = 0.5/12, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_x_continuous("Land-to-Land scores") +
  theme(legend.position = "bottom", axis.text.y = element_text(hjust = .5, angle = 90)) +
  geom_hline(yintercept = 0, show.legend = F)

grid.arrange(p1, p2, p3, nrow = 3, heights = c(1, 1, 1.4))
@

When evaluating classifier performance, it is common to use a receiver operating characteristic (ROC) curve, which plots the percent of wrong identifications against the percent correct identifications for each possible value of the cutoff between the two classes (in this case, known match and known non-match). Land-to-land scores are not perfectly separated, we can therefore use ROC curves to distinguish between the performance of the different methods and studies. The ROC curves for the land-to-land scores are shown in \autoref{fig:roc-auc} (a). A perfect classifier would have 100\% correct identifications and 0\% wrong identifications, e.g. the ROC curve would be a right angle at $(0, 100)$. Classifiers with better performance will be closer to this corner of the plot. A random classifier would have an ROC curve that was a straight diagonal line through $(0,0)$ and $(100,100)$.




<<rocs>>=
library(plotROC)
library(pROC)
rocpic <- all_lands %>% gather(measure, score, rfscore, ccf, cms) %>%
  mutate(
    measure = factor(measure, labels = c("CC", "CMS", "RF")),
    studyg = paste(study, group1) %>% str_trim %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))
  ) %>%
  ggplot(aes(linetype = measure, colour = studyg, shape = measure)) +
  geom_roc(aes(m = score, d = as.numeric(sameland)), 
           labels = FALSE, pointsize = 0, alpha = 0.8, size = 1) +
  theme_bw(base_size = 16) +
  # coord_fixed() + 
  facet_wrap(~studyg, nrow = 3) +
  xlab("Percent Wrong Identifications") +
  ylab("Percent Correct Identifications") +
  scale_colour_manual("Study", values = c("#fe9929", "#aa0000",
                                         "#74a9cf", "#2b8cbe", "#045a8d"),
                      guide=FALSE) +
  theme(legend.position= c(1, 0), legend.justification = c(1, 0), legend.direction = "vertical",
        legend.key.width = unit(1, "cm"), axis.text.y = element_text(angle = 90, hjust = .5)) +
  guides(linetype = guide_legend(override.aes = c(listsize = .5))) + 
  scale_linetype_manual("Measure", values=c(1,3,2)) +
  scale_shape_discrete("Measure") +
  scale_x_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "25%", "50%", "75%", "100%")) +
  scale_y_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "25%", "50%", "75%", "100%"))

@


<<aucs-pic, include = F>>=
eer <- function(roc){
  dframe <- data.frame(sens = roc$sensitivities, 
                       spec = roc$specificities,
                       threshold = roc$thresholds)
  dframe$err1 <- 1 - dframe$sens
  dframe$err2 <- 1 - dframe$spec
  dframe$diff <- abs(dframe$err1-dframe$err2)
  dframe$eer <- (dframe$err1+dframe$err2)/2
  idx <- which.min(dframe$diff)
  dframe[idx,c("eer", "threshold")]
}

aucs <- all_lands %>% gather(measure, score, rfscore, ccf, cms) %>% 
  group_by(study, group1, measure) %>% nest() %>%
  mutate(
    auc = data %>% purrr::map_dbl(function(d) {
      auc(d$sameland, d$score)
    }),
    auc_ci_low = data %>% purrr::map_dbl(function(d) {
      ci.auc(d$sameland, d$score)[1]
    }),
    auc_ci_high = data %>% purrr::map_dbl(function(d) {
      ci.auc(d$sameland, d$score)[3]
    }),
    roc = data %>% purrr::map(function(d) {
      roc(d$sameland, d$score)
    }),
    errors = roc %>% purrr::map(.f = eer)
  ) %>% 
  ungroup() %>% 
  mutate(
    studyg = paste(study, group1) %>% str_trim %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"),
                                                          labels = c("Hamby 44", "Phoenix PD", "Houston FSI\nG1", "Houston FSI\nG2", "Houston FSI\nG3")),
    measure = factor(measure, levels = c("cms", "ccf", "rfscore"), labels = c("CMS", "CC", "RF")),
    measurenum = as.numeric(measure),
    measurechar = as.character(measure)
  )
aucs <- aucs %>% unnest(errors)

aucpic <- aucs %>%
  ggplot(aes(x = auc, y = measurenum, colour = studyg)) +
  facet_grid(studyg~.) +
  geom_segment(aes(x = auc_ci_low, xend = auc_ci_low, y = measurenum - .125, yend = measurenum + .125), size = 1) + 
  geom_segment(aes(x = auc_ci_high, xend = auc_ci_high, y = measurenum - .125, yend = measurenum + .125), size = 1) + 
  geom_segment(aes(x=auc_ci_low, xend=auc_ci_high, y = measurenum, yend = measurenum, linetype=measure), size=1) +
  geom_point(size=3.5, aes(shape = measure)) +
  theme_bw(base_size = 16) +
  xlab("AUC and 95% confidence intervals") +
  xlim(c(0.5, 1)) +
  scale_y_continuous("", breaks = unique(aucs$measurenum), labels = unique(aucs$measurechar), limits=c(0.5, 3.5)) + 
  scale_colour_manual("Study", values = c("#fe9929", "#aa0000",
                                         "#74a9cf", "#2b8cbe", "#045a8d"),
                        guide = FALSE) +
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5)) + 
  # theme(legend.position = "bottom",
  #       legend.key.width = unit(1.5, "cm")) +
  scale_linetype_manual("Measure", values = c(3,1,2), guide = F) +
  scale_shape_discrete(guide=F)

@


<<roc-auc, echo = FALSE, out.width = c("0.51\\textwidth", ".4725\\textwidth"), out.height = c("0.63\\textwidth", "0.63\\textwidth"), fig.width = c(5, 6), fig.height = c(7.5, 7.5), fig.subcap=c("ROC curves", "Area under the curve (AUC)"), fig.cap="ROC curves and AUC for each (set) of the studies for the random forest score (RF), cross correlation (CC) and consecutively matching striae (CMS).   As seen in the ROC curves, the algorithm performs the least well on set 2 of the Houston FSI study. Based on AUC, the overall performance on all sets is very good to excellent for cross correlation and the random forest score. At the land level there is no significant difference in prediction power between RF score and cross correlation.", cache=FALSE>>=
  rocpic 
  aucpic
@

In \autoref{fig:roc-auc}a, the Houston FSI G1 and G3 curves and the Phoenix PD curve show excellent performance; Houston FSI G2 and Hamby 44 show still a very  good performance. Area under the curve (AUC) values, which summarize ROC curves, are shown in \autoref{fig:roc-auc}b. AUC values are useful for differentiating between poor, good, and excellent model performance, but are not particularly useful when determining which of several models with approximately the same level of performance should be used \citep{marzban_roc_2004}. Hamby 44 and Phoenix PD are the only studies with a somewhat noticeable difference between the AUC score for RF compared to cross correlation, but even those differences are not statistically significant.

\autoref{fig:eer} shows an overview of Equal Error and their thresholds based on the ROC curves of \autoref{fig:roc-auc}. Equal Errors are errors when sensitivity and specificity of a test are equal, i.e.\ we see the same percentage of missed and wrong identifications for a land-to-land comparison. Equal errors based on CMS of two lands are significantly higher than equal errors based on cross correlation or the RF score. At best, RF score and cross correlation have an equal error of around 5\% for land-to-land comparisons. 
Interestingly, for consecutively matching striae, equal errors are reached for just two matching striae, which from a practical point of view is not sustainable. A  CMS of  6 or above is an established cutoff used in identifications; when applied to the land-to-land comparisons, it is much more likely to result in a missed identification than a wrong identification. A threshold of 6 leads to 21 wrong identifications out of 49,286 different-source comparisons (0.04\% errors, or about 4 errors for each 10,000 evaluations); when applied to same-source comparisons, 517 of 715 same-source lands were incorrectly classified as exclusions, for an error rate of 80.6\%. 

<<eval=FALSE>>=
xtabs(~sameland+I(round(cms)>=6), data = all_lands)
@

<<eer, fig.width=6, fig.height = 2.6, fig.cap="Equal error (in Percent)  and corresponding thresholds for all measures (random forest score, cross correlation and CMS) under all five (sub-)studies. CMS has an equal error threshold of 2 for all studies, and perform signifcantly worse than both cross correlation and the random forest score. At best, Cross correlation and RF score have equal errors for land-to-land comparisons of around 5\\%. The performance of the random forest score is slightly better than the cross-correlation on sets which have higher equal error rates (Houston FSI G2, Hamby 44).", out.width = '\\textwidth'>>=
aucs %>% 
  mutate(
    eer = 100*eer,
    studyg = factor(studyg, labels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))
  ) %>%
  gather(eer_thresh, value, eer, threshold) %>%
  filter(!(measure=="CMS" & eer_thresh=="threshold")) %>%
  mutate(
    eer_thresh = factor(eer_thresh, labels=c("Equal Error (in Percent)", "Threshold for Equal Error"))
  ) %>%
  ggplot(aes(y = studyg, x = value, colour=measure, shape = measure)) + 
  geom_point(size = 3.5) +
  theme_bw() +
  xlab("Equal Error % (left) and corresponding thresholds (right)") +
  ylab("") +
  scale_color_brewer("Measure", palette = "Set2") +
  scale_shape_discrete("Measure", solid = F) + 
  theme(legend.position = "right") +
  facet_grid(.~eer_thresh, scales="free") +
  xlim(c(0, NA))
@


\section{Discussion and Conclusions}


At the beginning of this study, we anticipated Hamby 44 would be the easiest set to work with because of its similarity to the sets used to train the random forest algorithm. In a surprise turn of events, it was the hardest to solve, in part because of damage to the bullets that obscured LEA striae. It has been shown \citet{lpr2} that parts of lands can be used for successful identifications, if at least 50\% of the land is present (using full-length scores as the reference distribution). The random forest algorithm proposed in \citet{aoas2} is not capable of automatically detecting parts of lands with well-expressed striae. It may be useful to couple the \citet{aoas2} algorithm with an algorithm which assesses the quality of the input data and determines which portions of the data to use for comparison. This would emulate the process used by examiners, who first assess the quality of the evidence and whether there is enough information present to attempt a comparison. One major disadvantage of an automated algorithm is that all of the decisions humans make (recognizing degraded land areas, excluding those areas from consideration, matching only the remaining areas) must be explicitly characterized; however, this explicit characterization means that the process can be scientifically validated to a much higher degree than the human perceptual process. 

All of the studies presented here involve the same type of firearm. We see some variations in scores across different models of Rugers, and it is probable that we will see even bigger variations with different brands of firearms that vary in number of lands and land length. We also know that different firearms and ammunition combinations mark differently well \citep{boltonkingPreventingMiscarriagesJustice2016, bonfantiInfluenc1999}. Rugers are among the firearms that mark very well, so we would correspondingly expect to see lowered similarity scores for other firearms. While the performance of cross-correlation and random forest scores are similar when evaluated on Rugers, it is likely that the additional information used to generate the random forest score may be useful when comparing bullets from barrels which produce fewer distinctive marks. A future study will compare the performance of the random forest score, cross-correlation score, and CMS on non-Ruger barrels. The random forest algorithm is trained on scans taken at the land level, so barrels with polygonal rifling, such as Glocks, which do not introduce well defined lands on bullets, cannot be compared using the random forest algorithm examined here.  

The random forest matching algorithm presented in \citet{aoas2} does perform well on three different external test sets. While a single cutoff value cannot be used to distinguish matches and non-matches across the different test sets, the algorithm makes no errors when a set-specific cutoff value is used on bullet-to-bullet scores aggregated using sequence average maximum. This performance is on par with the performance of forensic examiners. For a future round of the Houston study, we are planning to compare the algorithmic performance directly with scores given by forensic toolmark examiners. The algorithm's score is not intended to replace an examiner's judgment; instead, it provides a complementary tool to measure, quantify, and compare the similarity of two bullets in an identification. With more research into the behavior of score-based likelihood ratios, this method may also be used to compute 
By validating the algorithm on external test sets, we have demonstrated that the method can be generalized to different types of ammunition and is not overly sensitive to small differences in rifling procedure. Future studies can and should generalize this to a wider range of external test sets to establish the limits of the algorithm's generalizability.


<<output-all-figs, include = F>>=
thisdoc <- readLines("index.Rmd")

# First, get figures that aren't in code chunks
# Haven't really tested this part - regex may need work
raw_figures <- data_frame(
  line_start = which(str_detect(thisdoc, "\\begin\\{figure\\}")), 
  line_end = which(str_detect(thisdoc, "\\end\\{figure\\}")),
  subfloats = purrr::map2(line_start, line_end, 
                          ~str_extract(thisdoc[.x:.y], "\\includegraphics(?:\\[.*\\])?\\{(.*)\\}") %>%
                            str_replace("\\includegraphics(?:\\[.*\\])?\\{(.*)\\}", "\\1"))
)

# Code chunk figures
chunk_figures <- data_frame(
  line_start = which(str_detect(thisdoc, "^```\\{r.*\\}$")),
  line_end = which(str_detect(thisdoc, "^``` *$")),
  chunk_name = str_replace(thisdoc[line_start], "(```\\{r ?)([0-9A-z-]*)(,?.*\\})", "\\2")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, chunk_name, ~thisdoc[.x] %>%
                              str_extract("fig\\.subcap ?= ?.*?\\),") %>%
                              str_replace("fig\\.subcap ?= ?", "") %>%
                              str_replace(",$", "") %>%
                              parse(text=.) %>% eval %>% length() %>% 
                              seq(1, ., by = 1) %>%
                              sprintf("figures/%s-%d.pdf", .y, .))
  )

# Knitr included graphics figures
chunk_incl_figures <- data_frame(
  line_start = which(str_detect(thisdoc, "^```\\{r.*\\}$")),
  line_end = which(str_detect(thisdoc, "^``` *$")),
  chunk_name = str_replace(thisdoc[line_start], "(```\\{r ?)([0-9A-z-]*)(,?.*\\})", "\\2")
) %>% 
  mutate(
    chunk_name = ifelse(chunk_name == "", "unnamed-chunk", chunk_name),
  ) %>% 
  mutate(
    subfloats = purrr::map2(line_start, line_end, function(.x, .y) {
      thisdoc[.x:.y] %>%
        paste(collapse = " ") %>%
        str_extract(., "include_graphics\\(c?\\(?.*?\\)?\\)") %>%
        str_replace("include_graphics\\(", "") %>%
        str_extract_all('\\"[\\.A-z0-9 /-]*?\\"') %>%
        parse(text = .) %>% eval() %>%
        str_remove_all("\\\\|\"")
    })
  )

# All figures
figs <- bind_rows(raw_figures, chunk_figures, chunk_incl_figures) %>%
  arrange(line_start) %>%
  mutate(subfloats = purrr::map(subfloats, function(x) data_frame(file = x, exists = file.exists(x)))) %>%
  tidyr::unnest() %>%
  filter(exists) %>%
  mutate(fig_num = group_indices(., line_start, line_end)) %>%
  group_by(fig_num) %>%
  mutate(rn = 1:n(),
         maxrn = n(),
         fig_label = letters[rn],
         fig_label = ifelse(maxrn > 1, fig_label, ""),
         fileext = tools::file_ext(file),
         fig_label = sprintf("Figure_%d%s.%s", fig_num, fig_label, fileext))

if (!dir.exists("separate_fig_files")) dir.create("separate_fig_files")
file.copy(figs$file, file.path("separate_fig_files", figs$fig_label), overwrite = T)

@
\section{References}

\bibliography{bibfile}

\end{document}

% <!--include marginal distributions of training data and compare to test data-->

<< echo=FALSE, eval=FALSE>>=
training <- read.csv("data/features-hamby.csv")
gtrain <- training %>% 
  select(profile1_id, profile2_id, study1, study2, match, ccf:sum_peaks) %>%
  gather(feature, value, ccf:sum_peaks)
# now compare to test data
@