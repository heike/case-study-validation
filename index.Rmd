---
title: Case study validations of automatic bullet matching

# to produce blinded version set to 1
blinded: 1

authors: 
- name: Heike Hofmann, Ph.D.
  affiliation: Department of Statistics, Iowa State University
  thanks: 
    - This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement \#70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia.
    - The authors wish to thank Jim Hamby, Tylor Klep (Phoenix PD), and Melissa Nally and Kasi Kirksey (FSI Houston) for access to test set bullets. XXX Scanning. Hamby advice.
  
- name: Susan VanderPlas, Ph.D.
  affiliation: Department of Statistics, Iowa State University

keywords:
- random forest
- 3d microscopy
- toolmark

abstract: Recent advances in microscopy have made it possible to collect 3d topographic data, enabling virtual comparisons based on the collected 3D data next to traditional comparison microscopy. Automatic matching algorithms have been introduced for various scenarios, such as matching cartridge cases (Tai and Eddy 2018) or matching bullet striae (Hare et al. 2017b,  Chu et al 2013,  De Kinder and Bonfanti 1999). One key aspect of validating automatic matching algorithms is to evaluate the performance of the algorithm on external tests. Here, we are presenting a discussion of the performance of the matching algorithm (Hare et al. 2017b) in three studies. We are considering matching performance based on the Random forest score, cross-correlation, and consecutive matching striae (CMS) at the land-to-land level and, using Sequential Average Maxima scores, also at the bullet-to bullet level. Cross-correlation and Random Forest scores both result in perfect discrimination of same-source and different-source bullets. At the land-to-land level, discrimination (based on area under the curve, AUC) is excellent ($ge$ 0.90). 

bibliography: bibliography
biblio-style: apsr

output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: template.tex
---

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{teal}{#1}}}
\noindent

\hh{
Cleanup TO-DO
\begin{itemize}
\item \sout{define false positives and false negatives once and stick to those definitions. We should avoid false positive and false negatives, because that only makes sense in a test setting, which we have not defined formally.
Let's use instead wrong identification and missed identification. }
\item \sout{error rates: we are using rates exchangeably with percentages. We need to fix that. I think percentages might be easier understood. XXX Should be fixed now}
\end{itemize}
}
\svp{
Outstanding XXX items:
\begin{itemize}
\item thanks - scanning and hamby advice
\item comparison to FTEs in conclusion
\end{itemize}
}


```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width = "\\textwidth",
  cache = TRUE,
  fig.path = "figures/",
  echo = FALSE
)
options(knitr.table.format = "latex")

library(tidyverse)
library(scales)
library(multidplyr) # install_github("hadley/multidplyr")
library(bulletxtrctr) # install_github("csafe-isu/bulletr")
library(gridExtra)
library(kableExtra)
```

```{r child-doc-compile, include = F}
library(stringr)
thisdoc <- readLines("index.Rmd")
idxs <- which(thisdoc == "---")
if (length(idxs) >= 2) {
  # Get yaml metadata, set to nonblinded, write to title-page.Rmd
  thisdoc[idxs[1]:idxs[2]] %>%
    str_replace("^blinded: 1", "blinded: 0") %>%
    writeLines(con = "title-page.Rmd")
}

rmarkdown::render("title-page.Rmd", clean = T)
```

```{r functions, eval=FALSE}
estimateBeta <- function(x, method = "mm", probs = c(0.25, 0.75)) {
  if (method == "mm") {
    mu <- mean(x, na.rm = TRUE)
    sd <- sd(x, na.rm = TRUE)
    lambda <- mu / sd * (1 - mu) / sd - 1
    alpha <- mu * lambda
    beta <- (1 - mu) * lambda
  }
  if (method == "LB") {
    quantile1 <- list(p = probs[1], x = quantile(probs = probs[1], x))
    quantile2 <- list(p = probs[2], x = quantile(probs = probs[2], x))
    bs <- LearnBayes::beta.select(quantile1, quantile2)
    alpha <- bs[1]
    beta <- bs[2]
  }
  return(c(alpha = alpha, beta = beta))
}
```

# Introduction and Background

In current practice, firearms and toolmark examiners (FTE) evaluate the similarity of striae on bullets by placing the evidence together with another bullet under a comparison microscope. The second bullet could be a test fire from a weapon recovered during the investigation, or it could be a second bullet from the crime scene. Examiners visually classify similarity according to the theory of firearms identification \citep{identification} as one of identification, inconclusive or exclusion. Exact guidelines for this classification vary from lab to lab; some labs will exclude only on the basis of non-matching class characteristics, such as direction of the twist in rifling, land length or number of lands, or type of rifling. In other labs, CMS (consecutively matching striae) as defined by \citet{biasotti} is used as a measure to quantify the strength of a match. In virtually all labs, individual characteristics used to identify matching bullets are derived from visual assessment; some class characteristics may be directly measured, but these are not sufficient for individualization.  <!-- Want to make it clear that individual characteristics are visually derived, rather than numerically derived... hopefully managed that without being insulting. HH: reads well -->

## Identification Using 3D Imaging Technology

More explicit characterization of bullet surfaces has been discussed since at least 1958 (\citeauthor{davis1968introduction}), but at the time technology was not sufficiently advanced to make an analysis based on 3D measurements or surface traces a viable option. \citet{ComputerIdentificationBullets1978} demonstrated use of a scanning electron microscope to quantitatively examine and compare bullet striae. More recently, approaches using 3D measurement data were explicitly described in 1999 (\citeauthor{dekinderAutomatedComparisonsBullet1999}), and have been further developed by \citet{bachrachDevelopment3DbasedAutomated2002,xieAutomatedBulletidentificationSystem2009, chuPilotStudyAutomated2010}. These approaches utilize 3D surface measurements directly to characterize the topology of land engraved areas (LEAs), rather than using visual or image comparison techniques. In many cases, these approaches also provide some level of automation of the comparison process, with the goal of reducing human biases by augmenting the visual information with 3D measurements. Utilizing the 3D measurements allows for examination of both peaks and valleys in LEAs, \hh{it also allows us to take the depth of striae into account, something that is difficult, if not impossible, for visual examination.} 

Commonly, approaches derived from 3D surface measurements use some features which are similar to those visually assessed by FTEs \citep{luAutomatedBulletIdentification2014}. Class characteristics, which are shared by a group of firearms with the same rifling design, manufacturer, and tooling process, are typically evaluated first, as a mismatch on class charcteristics is sufficient for an exclusion. Automated approaches to estimation of width of the land engraved area and twist angle were described in \citet{chuPilotStudyAutomated2010}. Individual characteristics, which are not shared by all members of a class, can also be automatically assessed from 3D surface measurements. One of the most common features used to assess the strength of a match based on individual characteristics is the cross-correlation function, which is utilized in several studies \citep{maNISTBulletSignature2004, vorburgerApplicationsCrosscorrelationFunctions2011, chuPilotStudyAutomated2010}. Additional features proposed for automatically assessing match strength also include signature distance \citep{maNISTBulletSignature2004}, striae depth and width \citep{ComputerIdentificationBullets1978}, and consecutive matching striae (CMS) \citep{chuAutomaticIdentificationBullet2013}.

## Automated Processing of 3D Scans

In the matching algorithm proposed in \citet{aoas}, several of the features described above are combined using a random forest \citep{breiman} to produce a similarity score based on 3d topographic scans of land engraved areas (LEAs).
From each land engraved area a signature is derived using the process described in detail in \citet{aoas}:

<!-- XXXX get from area to crosscut XXXX -->
<!-- \hh{ -->
<!-- Let $z(t)$ describe a digitized toolmark for the striation marks on a land engraved area. -->
<!-- $z(t)$ is then a spatial process indexed by $t$, where $t, t = 1,...,T$ denotes equally spaced pixel locations for a total length of $T$. } -->
\begin{itemize}
\item[(a)] Identify an area on the LEA with expressed striae. Locate a stable crosscut (see \citet{aoas}) in the identified area.
\item[(b)] Discard extraneous/contaminated data, such as data from groove engraved area and areas affected by break-off or contact with objects after the bullet exited the barrel  (``tank rash").
\item[(c)] Remove bullet curvature using a non-parametric smooth. 
\end{itemize}
A signature for a land engraved area is then defined as the sequence $S(x)$, $x = 1, ..., I$, where $I$ is the number of observed locations across the base of the bullet. \autoref{fig:signatures} shows a set of six signatures corresponding to the six land engraved areas of bullets 1 and 2 from barrel 1 of set 44 of the Hamby study \citep{hamby}. 
```{r signatures, fig.cap="Signatures of all six lands of bullets 1 and 2 from barrel 1 of the Hamby set 44.", fig.height=4, fig.width=10}
# signatures for one bullet
# h44cc <- readRDS("data/h44-ccdata.rda")
# br1 <- h44cc[1:12]
# dframe <- 1:12 %>% purrr::map_df(.f = function(i) {
#   d <- br1[[i]]
#   d <- d %>% group_by(y) %>% summarize(
#     value = mean(value, na.rm=TRUE)
#   )
#   d$id <- i
#   d %>% ungroup(y) %>% rename(
#     x = y
#   )
# })
#
# dframe$bullet <- rep(c("Bullet 1", "Bullet 2"), each = 6)[dframe$id]
# dframe$land <- paste("Land", (dframe$id %% 6 + 1))
#
# dframe <- dframe %>% group_by(bullet, land) %>% nest()
# names(dframe)[3] <- "ccdata"
#
# library(bulletxtrctr)
# dframe <- dframe %>% mutate(
#   grooves = ccdata %>%
#     purrr::map(.f = cc_locate_grooves, method = "middle",
#                adjust = 30, return_plot = TRUE)
# )
# dframe <- dframe %>% mutate(
#   sigs = purrr::map2(
#     .x = ccdata, .y = grooves,
#     .f = function(x, y) {
#       cc_get_signature(
#         ccdata = x, grooves = y, span1 = 0.75, span2 = 0.03)
#     })
# )
# dframe$barrelland <- dframe$land
# idx <- which(dframe$bullet=="Bullet 2")
# dframe$landid <- parse_number(dframe$land)
# dframe$landid[idx] <- (dframe$landid[idx] +1) %% 6 + 1
# dframe$land <- paste("Land", dframe$landid)
# signatures <- dframe %>% select(bullet, land, sigs) %>% tidyr::unnest()

signatures <- read.csv("data/signatures.csv")
signatures %>%
  ggplot(aes(x = x / 1000)) +
#  geom_line(aes(y = raw_sig), colour = "grey70") +
  geom_line(aes(y = sig), colour = "grey30") +
  facet_grid(bullet ~ land) +
  ylim(c(-5, 5)) +
  theme_bw() +
  ylab("Surface measurement (in microns)") +
  xlab("Relative Location (in millimeters)")
```

Once signatures have been extracted from stable regions with expressed striae, they must be aligned in order to assess their match strength. Maximized cross-correlation is used to pair-wise align signatures \citep{aoas,vorburgerApplicationsCrosscorrelationFunctions2011}. 

The cross-correlation (CC) function between two signatures $S_1(x)$ and $S_2(x)$ is defined as \[CC(S_1, S_2, z) = \text{cor}\left(S_1(x), S_2(x + z)\right)\] where $x$ and $x+z$ are integer values appropriately defined within the domains of $S_1$ and $S_2$ and  $z$ is the lag between the two sequences. When signatures contain missing values, pairwise complete observations are used to calculate the cross-correlation. The lag $z$ used to achieve maximum $CC$ is used to determine the best alignment.


## Statistics for Matching Aligned Signatures

Once two signatures are aligned, quantitative features describing the similarity of two signature are extracted. Features include characteristics of the signature that firearms and toolmark examiners base their visual assessment on, such as striae depth, number of matching striae, and the number of consecutively matching striae (CMS). Statistical features such as cross-correlation and Euclidean distance between the signatures are also extracted and used to assess the strength of the correspondence between the aligned signatures. 

One of the measures firearms examiners use to describe the strength of a match quantitatively is the number of consecutive matching striae (CMS) as defined by \citet{biasotti}. \svp{In order to evaluate CMS, we must first identify peaks and valleys in each of the signatures, then determine whether these peaks and valleys overlap sufficiently. Features which firearms examiners assess visually depend on the identification of these extrema; numerical features such as cross-correlation can be computed from the aligned signatures alone. } 

If there are at least 6 consecutive matching striae, the bullets are considered to be a good match \citep{biasotti_firearms_1997,nichols_scientific_2006}. 
CMS in this setting are counted as the number of consecutive peaks in signatures of two lands that match, in part because it is difficult to visually assess valleys except as relative to peaks. 
Note that \svp{with automatic alignment }based on the maximum cross-correlation rather than matching striation marks, it is possible for two lands to have a consecutive matching striae of 0. 
<!-- CMS are generally considered to be a very conservative criteria, in that bullets fired through the same barrel may not have a high number of consecutively matching striae; in this sense, CMS may be more likely to produce an inconclusive result. -->
<!--\hh{XXX for CMS we need to define peaks, and matching peaks, and for that we need to define the additional filter/smooth.}\svp{XXX Can't we just refer to the aoas paper? It seems to be defined in there relatively well.}-->
<!-- \svp{XXX Feels like we need to mention something about ccf here just because these are the 3 main features we're using to evaluate the algorithm...?} -->

\svp{Cross correlation can also be used to assess match strength between two aligned signatures; this cross-correlation value corresponds to the maximal cross-correlation over all valid lag values $z$. Cross correlation varies between -1 and 1, but as we are taking the maximal value, in practice the cross-correlations for aligned signatures are generally positive. Cross-correlation values which are close to 1 indicate signatures which are similar and may originate from the same source. Cross-correlation values which are close to 0 indicate signatures which may originate from different sources.}

The random forest presented in \citet{aoas} is based on a combination of multiple characteristics, including cross-correlation, number of matching striae, and number of consecutively matching striae. The output of the random forest is a score between 0 and 1 representing the algorithm's assessment of the strength of the match, where scores close to 1 indicate that the algorithm believes it to be highly likely that the aligned signatures come from the same source, while low scores indicate that the aligned signatures are highly likely to originate from different lands.

## Interpreting the Algorithm Score

\svp{For any of the features described above, high values over the expected range indicate that the signatures are more similar; low values indicate that the signatures are less similar. At some point along the range of possible values, an automated algorithm would produce a threshold - below that score, the signatures would be categorized as an exclusion; above it, the signatures would be categorized as an identification. }
<!-- Firearms examiners typically set the threshold for CMS at 6; this is a threshold which is conservative, as it is much more likely to exclude same-source comparisons than it is to identify different-source comparisons.  -->

We can enumerate characteristics of an ideal similarity scoring mechanism: 
\begin{description}
\item[(R1)] {\bf Monotonicity:} a higher score is indicative of higher similarity between a pair of bullets, in particular, similarity scores of same-source pairs of bullets are higher than different-source pairs.
\item[(R2)] {\bf Stability:} the same score leads to the same conclusion. 
\end{description}

\svp{In particular, requirement (R2) would imply that the same threshold value would be used for all comparisons of a certain scoring mechanism. If the same-source and different-source similarity score distributions overlap, setting a threshold value will introduce classification errors.}

\svp{This threshold system introduces a binary classification: identification or exclusion. This} \hh{definition of error is more stringent than the AFTE Theory of Identification, in that it does not allow for inconclusive results. } \svp{This increased rigor may increase the error rate of the model when compared to examiner error rates, but we expect that the increased information available from the 3D measurement data will compensate for this loss. A binary decision model is also easier to interpret and provides more clear-cut, definitive results.} 
<!-- \svp{It is likely that matches which were deemed  inconclusive would instead be marked as exclusions by the algorithm. XXX I don't know if we can make this statement, but I think it would help examiners understand how the line gets drawn if we could. I have exactly no evidence to back that up (except maybe the Hamby 44 tank rash results?)} -->


## Identification Errors and Algorithm Evaluation

When evaluating algorithm performance, it is useful to \hh{systematically assess }\svp{ the set of possible outcomes}. If ground truth and the algorithm's prediction match, we have either a correct identification or a correct exclusion. If ground truth and the algorithm's prediction do not match, we distinguish between two types of errors, which we will refer to as \textbf{wrong identification} and \textbf{missed identification}. Wrong identifications are those in which two bullets from different sources are determined to be from the same source. Missed identifications are those in which two bullets from the same source are determined to be from different sources. The full range of possible outcomes is shown in \autoref{tab:possible-outcomes}.
\begin{table}
\centering
\begin{tabular}{c|ccc}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Ground Truth}}\\\cline{3-4}
\multicolumn{2}{c}{} & \multicolumn{1}{|c}{Same Source} & \multicolumn{1}{|c|}{Different Source} \\\cline{2-4}
\multirow{8}{*}{\rotatebox[origin=c]{90}{\hfill \textbf{Results}\hspace{6mm}\hfill}} & 
  \multirow{4}{*}{\rotatebox[origin=c]{90}{\hspace{1mm}Exclusion\hspace{1mm}}} & \multicolumn{1}{|c}{} & \multicolumn{1}{|c|}{}\\
  & &  \multicolumn{1}{|c}{Missed} & \multicolumn{1}{|c|}{Correct}\\
  & &  \multicolumn{1}{|c}{Identification} & \multicolumn{1}{|c|}{Exclusion}\\
  & &  \multicolumn{1}{|c}{} & \multicolumn{1}{|c|}{} \\\cline{2-4}
& \multirow{4}{*}{\rotatebox[origin=c]{90}{\hspace{1mm}Identification\hspace{1mm}}}  & \multicolumn{1}{|c}{} & \multicolumn{1}{|c|}{}\\
  & &  \multicolumn{1}{|c}{Correct} & \multicolumn{1}{|c|}{Wrong}\\
  & &  \multicolumn{1}{|c}{Identification} & \multicolumn{1}{|c|}{Identification}\\
  & &  \multicolumn{1}{|c}{} & \multicolumn{1}{|c|}{} \\\cline{2-4}
\end{tabular}
\caption{Possible outcomes of an examination of two pieces of evidence. Correct decisions are shown in the top-right and bottom-left corners; incorrect decisions in the top-left and bottom-right corners.\label{tab:possible-outcomes}}
\end{table}


\svp{In machine learning and statistics, it is common to evaluate algorithms using sensitivity and specificity; these concepts are related to the error rate. \textbf{Sensitivity} is defined as the proportion of actual positives that are correctly identified; that is, when ground truth is same source, the sensitivity is the proportion of correct identifications. \textbf{Specificity} is defined as the proportion of actual negatives that are correctly identified; that is, when ground truth is different source, the specificity is the proportion of correct identifications. The combination of sensitivity and specificity is sufficient to describe the reliability of an algorithm. }

\svp{When evaluating the \citet{aoas} algorithm, we will describe errors as a percentage of correct evaluations. Statistics that do not translate to percent-form interpretation, such as area under the curve (AUC), will be presented as a decimal. }



## Assessing Match Strength

\svp{Each of the scoring methods we have described is computed on a land-to-land basis.} While these comparisons are useful, \hh{the question of interest typically involves }  the entire physical object (e.g. all lands on a single bullet), \hh{and a conclusion on same or different source should be reached based on the evidence of all lands of the bullets}.
<!--to describe the correspondence between two pieces of physical evidence.-->
<!-- \svp{In order to aggregate land-to-land match scores, we must consider the physical structure of the evidence; that is, that the sequence of lands stems from spatial information, but the numbering of each land represents the scan order and start point rather than encoding useful information. } --><!--This last bit might be redundant-->

```{r, echo=FALSE, results = 'hide', warning = FALSE, message = FALSE}
features <- read.csv("data/fsi-features.csv.gz")
features <- features %>% mutate(
  bullet1 = gsub("ullet ", "", bullet1),
  bullet2 = gsub("ullet ", "", bullet2)
)
b12 <- features %>% filter(
  group1 == "G1", barrel1 == "KA", bullet1 == "B1",
  group2 == "G1", barrel2 == "KA", bullet2 == "B2"
)
p1 <- b12 %>%
  ggplot(aes(x = land1, y = land2, fill = rfscore)) +
  geom_tile() + ggtitle("Same barrel") +
  scale_fill_gradient2("RF score: ", midpoint = 0.3, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  theme_bw() +
  xlab("Bullet 2") + ylab("Bullet 1") +
  theme(legend.position = "bottom") + coord_equal()

bab <- features %>% filter(
  group1 == "G1", barrel1 == "KA", bullet1 == "B1",
  group2 == "G1", barrel2 == "KB", bullet2 == "B1"
)
p2 <- bab %>%
  ggplot(aes(x = land1, y = land2, fill = rfscore)) +
  geom_tile() + ggtitle("Different barrel") +
  scale_fill_gradient2("RF score: ", midpoint = 0.3, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  theme_bw() +
  xlab("Bullet 2") + ylab("Bullet 1") +
  theme(legend.position = "bottom") + coord_equal()
```

Land-to-land comparisons lead to a whole set of scores for bullet-to-bullet comparisons. \autoref{fig:b2b} shows two matrices of scores for \hh{two pairs} of bullet-to-bullet matches. On the left, a matrix is shown that is typical for scores from two bullets from the same barrel. On the right, values for a pair of known non-matching bullets are shown.


```{r b2b, echo = FALSE, fig.width = 9, fig.height = 5,  out.width='.6\\textwidth', fig.cap="Overview of land-land matching scores for two pairs of bullet-bullet comparisons. On the left the two bullets are known to come from the same source (barrel) on the right, the bullets are from two different sources (barrels)"}
grid.arrange(p1, p2, ncol = 2, padding = 2)
```

When imaging bullets, operators scan one land at a time in a clockwise (left twisted rifling) or anti-clockwise (right twisted rifling) sequence. The order in which scans are acquired is kept as meta-information. Let us assume that lands on a bullet are labelled $\ell_i$ with $i = 1, .., p$, \hh{where $p$ indicates the number of lands a bullet has, as determined by the rifling of a barrel. For all of the bullets considered here the number of lands $p$ is 6}. A match between  \hh{a pair of lands from two} bullets therefore results in an expected additional $p-1$ matches between pairs of lands. These lands are also expected to be in a sequence, i.e. if there is a match between lands $\ell_i$ on bullet 1 and $\ell_j$ on bullet 2, we also expect lands $\ell_{i\oplus s}$ and $\ell_{j\oplus s}$ to match for all integers $s$, where $\oplus$ is defined as $a \oplus b \equiv \left((a + b - 1)\mod p\right) + 1$. This relationship gives rise to the sequence average maximum (SAM) to quantify a bullet-to-bullet match. 

\begin{definition}{Sequence Average and its Maximum}
Let $A$ be a square real-valued matrix of dimensions $p \times p$. For the purpose of this paper, $A$ consists of scores describing the similarity between two sets of land engraved areas.
The $k$th \emph{sequence average} $SA(A, k)$ for $k = 0, ... p-1$ is defined  as 
$$SA(A, k) = \frac{1}{p} \sum_{i=1}^{p} a_{i,i \oplus k}, \text{ where } i\oplus k := \left((i + k - 1)\mod p\right) + 1.$$
The \emph{Sequence Average Maximum} \citep[SAM, ][]{sam} of square matrix $A$ of scores is defined as
$$SAM (A) = \max_{k = 1}^{p} SA(A, k).$$
\end{definition}

Note that the sequence average maximum of the correlation between lands is used in SensoComp to capture the similarity between bullets. The correlation based SAM score has also been called the 'average correlation calculated at the max phase' in \citet{chuPilotStudyAutomated2010}. 

```{r sam-sketch, out.width='\\textwidth', fig.width=12, fig.height = 2.5, fig.cap="Sketch of all six land-to-land sequences between two bullets with six lands. "}
df2 <- data.frame(expand.grid(x = 1:6, y = 1:6))
df2$case1 <- df2$x == ((df2$y + 5) %% 6) + 1
df2$case2 <- df2$x == ((df2$y + 0) %% 6) + 1
df2$case3 <- df2$x == ((df2$y + 1) %% 6) + 1
df2$case4 <- df2$x == ((df2$y + 2) %% 6) + 1
df2$case5 <- df2$x == ((df2$y + 3) %% 6) + 1
df2$case6 <- df2$x == ((df2$y + 4) %% 6) + 1

df2 %>%
  gather("case", "values", starts_with("case")) %>%
  mutate(case = gsub("case", "", case)) %>%
  mutate(case = paste0("k = ", as.numeric(case) - 1)) %>%
  ggplot(aes(x = factor(x), y = factor(y, levels = 1:6), fill = values)) +
  geom_tile(colour = "grey20") +
  facet_wrap(~case, ncol = 6) +
  xlab("Bullet 2") + ylab("Bullet 1") +
  scale_x_discrete(labels = paste0("L", 1:6)) +
  scale_y_discrete(labels = paste0("L", 1:6)) +
  scale_fill_manual("Cell included in SA(A,k)", values = c("white", "grey20")) +
  coord_equal() +
  theme(legend.position = "bottom")
```

Looking back at \autoref{fig:b2b}, we see that for the two bullets from the same barrel, the sequence average for 
$k=2$ is higher than the other sequence averages, and also higher than the sequence averages for the other pair of bullets shown on the right of the figure.

SAM scores allow us to define a single quantity for each pair of bullets that describes the similarity between these two bullets. 


In this paper, we validate the algorithm described in \citet{aoas} on three external test sets. We assess the results on each set and evaluate whether the minimal requirements described above are fulfilled for bullet-to-bullet SAM scores. We compare the random forest algorithm's performance to the performance of other suggested measures for quantitative assessment of match strength, such as cross-correlation and consecutive matching striae. 

# Validation Sets

Random forests \citep{breiman} have a built-in internal testing mechanism to prevent potential overfitting. Errors reported by random forest algorithms are based on these internal test sets. However, there is some discussion on whether or in which situations these errors are biased \citep{rfOver2, rfOver}. 
Neither of these papers addresses another issue with internal test sets: internal test sets are constructed to have the same distribution as the training data (apart from sampling variability). For any machine learning method, a true benchmark of the performance of an algorithm requires testing its performance on external test data. Good performance (in terms of wrong and missed identifications) on external test data validates a ML algorithm. External data also allows an assessment of the algorithm's sensitivity to distributional changes as well as testing the algorithm's robustness by going outside the parameters of the training data.  

The algorithm in \citet{aoas} is trained on scans from Hamby sets 252 and 173 made available through \citet{nist}. Set 173 was originally published as Hamby set 44. This mis-labeling has been corrected now. The scans used to train the model were taken at 20 fold magnification for a resolution of 1.5625 microns per pixel. While magnification is generally of interest in microscopy, **resolution** -- usually measured in microns per pixel -- is of more interest for 3D topographic measurements, as it determines the operative level of available data.


For the validation of the automatic matching algorithm we are considering three validation sets of -- what should be -- increasing difficulty level: 
\begin{itemize}
\item {\bf Hamby set 44} is one set of the Hamby study \citep{hamby}. Each Hamby set  consists of a total of 35 bullets fired through ten consecutively manufactured barrels of Rugers P85. Each set consists of 20 known bullets (two from each of the ten barrels) and 15 questioned bullets of unknown origin. Note that all Hamby sets are closed sets; that is, all questioned bullets are fired through one of the ten barrels. The ammunition used for this set were 9 mm Luger 115 Grain Full Metal Jacket  from the Winchester Ammunition Company.
\item {\bf Phoenix PD}
Tylor Klep from Phoenix PD provided sets of known test fires and questioned bullets: the set of known bullets consists of three test fires (B1, B2, B3) from each of eight different, consecutively manufactured Ruger P-95 barrels (A9, C8, F6, L5, M2, P7, R3, U10). Ten questioned bullets were provided (B, E, H, J, K N, Q, T, Y, Z). This set is an open set; that is,\ it is not known in advance whether all (or any) of the questioned bullets are fired from the known barrels. \hh{In fact, the results will show that three of the questioned bullets were fired from three different barrels not included in the knowns. These three barrels correspond to an eleventh Ruger P-95, a Ruger P-95C and a Ruger P-85.} \svp{All bullets fired for this study are American Eagle 9mm  copper jacketed Luger.}
Land engraved areas for each of the six lands of each bullet were scanned by Bill Henderson (Sensofar).
\item {\bf Houston FSI}
This study was set up by Melissa Nally and Kasi Kirksey from FSI Houston. Three test sets based on ten consecutively rifled Ruger LCP barrels (A, B, C, D, E, F, G, H, I, J) and three other, non-consecutively rifled Ruger LCP barrels (R1, R2, R3). Each test set consists of three test fires each from five consecutively rifled barrels. Additionally, ten questioned bullets are provided for each kit. The ammunition used in both test fires and the questioned bullets were Remington UMC 9mm Luger Full Metal Jackets. All three of the test sets are open; that is,\ not every one of the questioned bullets is fired from the five known barrels in each of the test set.
The structure of these three sets is similar to a forthcoming study by Nally and Kirksey, but the results here come from preliminary test sets made available to us.
\end{itemize}

Scans of all land engraved areas for the validation data were taken on a Sensofar Confocal Light microscope at 20x magnification resulting in a resolution of 0.645 microns per pixel. If not indicated otherwise, scans were taken at the Roy J Carver high resolution microscopy lab at Iowa State University.

Case studies were chosen such that difficulty for the matching algorithm increases with each case study: Hamby set 44 is part of the Hamby study. The algorithm in \citet{aoas} was trained on sets 173 and 252, so the bullets in Hamby 44 are of the same type of ammunition and are fired through the same barrels as the bullets in the training set. The Phoenix PD set uses Ruger P-95 barrels, which are different from the barrels in the Hamby sets used for training the algorithm, but the barrels are rifled similarly to the Ruger P-85 barrels. The Houston sets use Ruger LCP barrels. These barrels are first rifled traditionally, i.e. similar to the Ruger P-85, but are treated to a secondary round of heating after rifling. This second round of heat exposure has the potential to introduce some subclass characteristics, i.e. anomalies that are shared between different barrels manufactured in this manner. This should make the automatic matching harder, and in particular, is expected to complicate the classification as different-source land engraved areas/bullets. In all three studies, the scan resolution is much higher than the scans used to train the algorithm in \citet{aoas}; this difference also provides a test of the algorithm's ability to generalize to scans taken at different resolutions.


# Results

## Hamby Set 44

```{r h44, echo=FALSE, fig.width = 10, fig.height=5, fig.cap="Hamby set 44: overview of all bullet-to-bullet matches between all pairs of questioned bullets ($y$-axis) and known test fires from ten barrels ($x$ axis)."}
h44features <- read.csv("data/h44-features.csv")
h44features <- h44features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)
# tank rash lands:
#h44features <- h44features %>% mutate(
#  rfscore = replace(rfscore, (bullet1 == "I" & land1 == 6) | (bullet2 == #"I" & land2 == 6), NA)
#)
h44features <- h44features %>% mutate(
  rfscore = replace(rfscore, (barrel1 == 8 & bullet1 == "1" & land1 == 6) | (barrel2 == 8 & bullet2 == "1" & land2 == 6), NA)
)

h44nest <- h44features %>%
  filter(barrel1 == "Unk") %>%
  filter(barrel1 != barrel2 | bullet1 != bullet2) %>%
  mutate(
    barrel1 = "Unknown",
    barrel2 = as.character(barrel2),
    barrel2 = replace(barrel2, barrel2 == "Unk", "Unknown")
  ) %>%
  group_by(bullet1, barrel2, bullet2) %>%
  nest()

h44nest <- h44nest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)
h44nest <- h44nest %>%
  mutate(
    barrel2 = factor(barrel2, levels = c(1:10, "Unknown")),
    bullet1 = factor(bullet1, levels = rev(c(
      "K", "O", "L", "P",
      "J",
      "H", "I", "Y", "G",
      "E", "U", "X", "F",
      "T", "S"
    ))),
    bullet2 = factor(bullet2, levels = c(1:2, rev(levels(bullet1))))
  )
h44 <- h44nest %>%
  ggplot(aes(
    y = bullet1, x = bullet2,
    fill = sam_rf
  )) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, space = "free", scales = "free") +
  ylab("Questioned bullets") +
  xlab("Test fires from barrels 1 to 10 (left), questioned bullets (right)") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.35, limits = c(0, 1)
  ) +
  scale_colour_manual("same source", values = "darkorange") +

  #  scale_x_discrete(position = "top") +
  scale_y_discrete() +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(h44nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Hamby set 44") +
  theme_bw() +
  theme(legend.position = "bottom")


h44nest$samesource <- h44nest$sam_rf > 0.3

h44
``` 

\autoref{fig:h44} shows an overview of all scores from pairs of questioned bullets with all other bullets. On the left of \autoref{fig:h44} there are ten strips labelled 1 through 10. These strips correspond to known barrels 1 through 10. \svp{Each barrel was test-fired twice, so each of the questioned bullets (shown along the $y$ axis) matches two bullets fired from a known barrel. Colored tiles are used to show the strength of the comparison; light grey colors correspond to low similarity scores, dark colors correspond to high similarity scores.  squares with a colored frame indicate ground truth. } Ground truth is encoded in this figure as a thin, dark, colored frame for all pairs of same-source bullets. 
\svp{We want to see one barrel with two dark-filled, dark-framed tiles for each questioned bullet, and light grey tiles for all other barrels, indicating a match between a questioned bullet and a single barrel.} This expectation is met for all questioned bullets, i.e. the automatic matching  identifies the correct barrel for all questioned bullets. For two of the questioned bullets, 'I' and 'F', the similarity scores to the matching barrels are considerably smaller than for the other questioned bullets. 

The right side of \autoref{fig:h44} shows the relationship between all pairs of questioned bullets. Note that questioned bullets are not compared to themselves, leaving white squares on the diagonal. Some of the questioned bullets match the same barrel, e.g.\ questioned bullets 'P' and 'J' both match barrel 5. Therefore bullets 'P' and 'J' also match each other in the square on the right hand side.

```{r close, eval=FALSE, fig.width=11.25, fig.height=4.2, fig.cap="Examples of land-to-land comparisons from the Hamby-44 set. Along the top are the three bullets from different source with the highest RF scores, along the bottom are the three bullets from same source with the lowest RF score.", fig.subcap=c("Ground truth: different source", "Ground truth: same-source"), fig.align="middle", fig.ncol=1}
# ex2 <- h44nest %>% filter(samesource == FALSE & sam_rf >= 0.287)
# rfs2 <- ex2$sam_rf
# ex2 <- ex2 %>% mutate(
#   data = data %>% purrr::map(.f = function(d) {
#  #   browser()
#     index <- which.max(bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore))
#     d$same <- ((d$land2 - d$land1) %% 6)  == index-1
#
#   #   d %>% ggplot(aes(x = land1, y=land2, fill=rfscore)) +
#   #                  geom_tile() +
#   #     geom_tile(colour="darkorange", data = d %>% filter(same), size=0.5) +
#   #     scale_fill_gradient2(low="darkgrey", high = "darkorange",
#   #                      midpoint = 0.5) +
#   #     scale_colour_manual(values=c( "darkorange")) +
#   # scale_x_discrete("") +
#   # scale_y_discrete("")
#     d
#   })
# )
# ex2$example <- with(ex2, paste0(bullet1, " vs ", barrel2, "-", bullet2))
# ex2 <- unnest(ex2, data)
# ex2 %>%
#   ggplot(aes(x = land1, y = land2, fill=rfscore)) +
#   geom_tile() +
#   theme_bw() +
#   scale_fill_gradient2("RF score", low="darkgrey", high = "darkorange",
#                        midpoint = 0.5) +
# #  geom_tile(colour="darkgrey", data = ex2 %>% filter(same), size=0.5) +
#   facet_grid(.~example) +
#   scale_x_discrete("") +
#   scale_y_discrete("") +
#   annotate(geom="text", x = 0.5, y = 6.75, label=sprintf("SAM RF: %.3f", rfs2), hjust = 0)
#
#
#
# ex1 <- h44nest %>% filter(samesource == TRUE & sam_rf < 0.4)
# rfs <- ex1$sam_rf
# ex1 <- ex1 %>% mutate(
#   data = data %>% purrr::map(.f = function(d) {
#  #   browser()
#     index <- which.max(bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore))
#     d$same <- ((d$land2 - d$land1) %% 6)  == index-1
#
#   #   d %>% ggplot(aes(x = land1, y=land2, fill=rfscore)) +
#   #                  geom_tile() +
#   #     geom_tile(colour="darkorange", data = d %>% filter(same), size=0.5) +
#   #     scale_fill_gradient2(low="darkgrey", high = "darkorange",
#   #                      midpoint = 0.5) +
#   #     scale_colour_manual(values=c( "darkorange")) +
#   # scale_x_discrete("") +
#   # scale_y_discrete("")
#     d
#   })
# )
# ex1$example <- with(ex1, paste0(bullet1, " vs ", barrel2, "-", bullet2))
# ex1 <- unnest(ex1, data)
# ex1 %>%
#   ggplot(aes(x = land1, y = land2, fill=rfscore)) +
#   geom_tile() +
#   theme_bw() +
#   scale_fill_gradient2("RF score", low="darkgrey", high = "darkorange",
#                        midpoint = 0.5, na.value = NA) +
#   geom_tile(colour="darkorange", data = ex1 %>% filter(same), size=0.5) +
#   facet_grid(.~example) +
#   scale_x_discrete("") +
#   scale_y_discrete("") +
#   annotate(geom="text", x = 0.5, y = 6.75, label=sprintf("SAM RF: %.3f", rfs), hjust = 0)


# \autoref{fig:close} shows an example of six comparisons of bullet pairs at the land-to-land level. The panels along the top of the figure show three bullet pairs that are known to be from different barrels, the three bullet pairs along the bottom are known to be from the same barrel. The light orange frame in the bottom three panels indicate which lands correspond to one-another in the matching. It is obvious that not all of the scores are as high as we would usually see in same-source comparisons.
```

## Phoenix PD

```{r pd, echo=FALSE, fig.width = 9, fig.height=4, fig.cap="Phoenix study: overview of all bullet-to-bullet matches between all pairs of questioned bullets ($y$-axis) and known test fires from ten barrels ($x$ axis). The order of the bullets on the $y$ axis is determined by matching barrel."}
features <- read.csv("data/pd-features.csv.gz")
features <- features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)
# change labels for land 3 and land 4 of bullet B3 in P7
features <- features %>% mutate(
  first = as.character(first),
  first = replace(first, first=="Gun 1-P7/B3/L4.dat", "XXX"),
  first = replace(first, first=="Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"),
  first = replace(first, first=="XXX", "Gun 1-P7/B3/L3.dat")
)

features <- features %>% mutate(
  KM = replace(KM, first %in% c("Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"), FALSE)
)
features <- features %>% mutate(
  KM = ifelse(first == "Gun 1-P7/B3/L3.dat", (second == "Unknown 1-J/L3.dat"), KM),
  KM = ifelse(first == "Gun 1-P7/B3/L4.dat", (second == "Unknown 1-J/L4.dat"), KM)
)



f2 <- features %>%
  separate(first, into = c("foo1", "foo2", "barrel1", "bullet1", "land1", "foo3"), remove = FALSE) %>%
  separate(second, into = c("foo4", "foo5", "bullet2", "land2", "foo6"), remove = FALSE) %>%
  select(-foo1, -foo2, -foo3, -foo4, -foo5, -foo6)


unknowns <- read.csv("data/pd-features-unknown.csv")
unknowns <- unknowns %>% filter(bullet1 != bullet2)
unknowns <- unknowns %>% mutate(
  bullet2 = gsub("Unknown 1-", "", bullet2),
  bullet1 = gsub("Unknown 1-", "", bullet1)
)
f3 <- rbind(
  f2 %>%
    select(
      b1, b2, barrel1, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, KM
    ) %>%
    mutate(barrel2 = "Unknown"),
  unknowns %>%
    select(
      b1, b2, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, 
    ) %>%
    mutate(barrel1 = "Unknown", barrel2 = "Unknown", KM = FALSE)
)

pdnest <- f3 %>% group_by(bullet2, barrel1, bullet1) %>% nest()

pdnest <- pdnest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  KM = data %>% purrr::map_lgl(.f = function(d) {
    any(d$KM)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)

pdnest <- pdnest %>%
  mutate(
    bullet2 = factor(bullet2, levels = rev(c("N", "B", "E", "T", "H", "J", "K", "Q", "Y", "Z"))),
    bullet1 = factor(bullet1, levels = c("B1", "B2", "B3", rev(levels(bullet2))))
  )
p3 <- pdnest %>%
  ggplot(aes(y = bullet2, x = bullet1, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel1, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.45, limits = c(0, 1)
  ) +
  scale_colour_manual("same source", values = "darkorange") +
  scale_y_discrete() +
  theme_bw() +
  theme(legend.position = "bottom") +
  #  coord_equal() +
  geom_tile(aes(colour = TRUE),
    size = .5,
    data = filter(pdnest, KM)
  ) +
  guides(colour = guide_legend(override.aes = list(fill = NA)))


pdnest$samesource <- pdnest$KM

p3 + ggtitle("Phoenix PD set")
```

\autoref{fig:pd} shows an overview of similarity scores for all pairs of questioned bullets and  test fires. The color encoding is the same as in the previous figures. Here, we see that questioned bullets either match all 3 bullets fired out of the same barrel for exactly one of the barrels or none of the known barrels. Questioned bullets 'Q', 'Y', and 'Z', do not match any of the known barrels.
None of the test fires from barrel U10 match any of the questioned bullets.  This is a sign of the previously mentioned open set characteristic of the study. 
Once again, the results from automatic matching correctly pair questioned bullets with their corresponding barrels.

## Houston FSI

\autoref{fig:hou} shows an overview of the three sets of scores for all pairs of questioned bullets with all other bullets in the set. In set 1, five of the questioned bullets can be matched to known bullets in the set. None of the questioned bullets in set 1 match bullets fired from barrels KC or KD. Additionally, two of the questioned bullets which do not match any of the barrels in the set match each other. In set 2, four of the questioned bullets can be matched to known bullets in the set; of the additional four questioned bullets which do not match any known bullets, two were fired from the same barrel. In set 3, six of the questioned bullets match known bullets, and the remaining two questioned bullets match each other but do not match any known bullets in the set. 
<!--Thus, we can conclude that the algorithm performs well on open set studies, as it can identify matches among questioned bullets which do not match any known bullets. -->
While all bullet-to-bullet matches are correctly identified, and no known matches are missed, it is obvious from the generally lighter shades of the tiles corresponding to matching bullets in \autoref{fig:hou}\svp{(b) that the algorithm is not performing as well on set 2 as it performs on sets 1 and 3.}
```{r hou, fig.width=9, fig.height = 3, fig.cap="Overview of matching scores for all pairs of questioned bullets to known bullets from five barrels (on the left) and questioned bullets to themselves (tiles on the right).", fig.subcap=c("Set 1", "Set 2", "Set 3"), fig.align="middle", fig.ncol=1}
features <- read.csv("data/fsi-features.csv.gz")
features <- features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)

features <- features %>% mutate(
  bullet1 = gsub("ullet ", "", bullet1),
  bullet2 = gsub("ullet ", "", bullet2)
)

features <- features %>%
  filter(group1 == group2) %>%
  filter(barrel1 == "Unknowns") %>%
  filter(barrel1 != barrel2 | bullet1 != bullet2) 

groups <- features %>%
  group_by(group1, barrel1, bullet1, barrel2, bullet2) %>%
  nest()


hounest <- groups %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)

hounest$samesource <- hounest$sam_rf > 0.3

hou1nest <- hounest %>%
  filter(group1 == "G1") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U40", "U36", "U42", "U10", "U77", "U37", "U28", "U15"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )

hou1 <- hou1nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou1nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 1")


hou2nest <- hounest %>%
  filter(group1 == "G2") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U23", "U41", "U61", "U98", "U73", "U34", "U63", "U66"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )
hou2 <- hou2nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou2nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 2")

hou3nest <- hounest %>%
  filter(group1 == "G3") %>%
  mutate(
    bullet1 = factor(bullet1, levels = rev(c("U27", "U33", "U36", "U49", "U56", "U65", "U14", "U45"))),
    bullet2 = factor(bullet2, levels = c("B1", "B2", "B3", levels(bullet1)))
  )

hou3 <- hou3nest %>%
  ggplot(aes(y = bullet1, x = bullet2, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel2, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.375, limits = c(0, 1)
  ) +
  scale_y_discrete() +
  scale_colour_manual("same source", values = "darkorange") +
  geom_tile(aes(colour = TRUE), size = .5, data = filter(hou3nest, sam_rf > 0.3)) +
  guides(colour = guide_legend(override.aes = list(fill = NA))) + ggtitle("Houston set 3")

hou1

hou2

hou3
```


# Evaluating the Random Forest Algorithm
\svp{The random forest SAM scores correctly separate known matches from known non-matches, using SAM scores to aggregate land-to-land scores into a bullet-to-bullet comparison. With no errors at the bullet-to-bullet level, we now need to evaluate the scores in light of (R1) and (R2). In this section, we will compare the random forest scores to cross-correlation scores; cross-correlation is one of the components of the random forest, but has also been used to quantify the strength of a match in its own right \citep{chuPilotStudyAutomated2010}. In addition, both scoring methods use the same scale (0 to 1) and are continuous along that interval; other evaluation methods, such as consecutive matching striae are discrete and more difficult to directly compare to continuous measurements.}

<!-- \svp{XXX We may want to add in SAM scores for CMS as well, but that's going to make the the scales on the density plots (and the density plots themselves) a bit wonky. Thoughts?} -->

## Comparison of SAM scores
\autoref{fig:compare} shows density curves for the scores bullet-to-bullet matches of each of the studies, comparing the SAM scores based on cross-correlation (top) and the random forest score (bottom). Color indicates ground truth. Small vertical lines below the $x$-axis indicate observed scores in a **rug-plot**. The plot shows that for all three case studies and both continuous measures, i.e. SAM scores based on cross-correlation as well as SAM scores based on the random forest scores, all similarity scores are higher for pairs of bullets from the same source, i.e. bullets fired through the same barrel, than pairs of bullets from different sources (fired through different barrels). 
 
```{r compare, fig.width = 8, fig.height = 5, fig.cap="Density curves of similarity scores from cross-correlation (top) and RF scores (bottom). Different colors indicate same source versus different source. Ideally all scores of different source pairs should be much lower than scores for same source pairs."}
nests <- rbind(
  h44nest %>%
    select(sam_rf, sam_ccf, samesource) %>%
    mutate(study = "Hamby 44", group1 = NA),
  pdnest %>%
    select(sam_rf, sam_ccf, samesource) %>%
    mutate(study = "Phoenix PD", group1 = NA),
  hounest %>%
    select(sam_rf, sam_ccf,  samesource, group1) %>%
    mutate(study = "Houston FSI")
)
nests$study <- factor(nests$study, levels = c("Hamby 44", "Phoenix PD", "Houston FSI"))
nestslong <- nests %>% gather(key = "measure", value = "value", sam_rf, sam_ccf)
nestslong$measure <- factor(nestslong$measure)
levels(nestslong$measure) <- c("Cross-correlation",  "RF score")

labels <- h44nest %>% filter(samesource, sam_rf < 0.5)
labels$comparison <- with(labels, paste(bullet1, "vs", paste(barrel2, bullet2, sep = "-")))
labels$measure <- "RF score"
labels$study <- "Hamby 44"
labels$y <- 5 + 4:1
nestslong %>%
  ggplot(aes(
    x = value, fill = samesource,
    colour = samesource
  )) +
  geom_density(alpha = 0.6) +
  geom_rug(alpha = 0.5, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_x_continuous("SAM scores", limits=c(0,1)) +
  theme(legend.position = "bottom") +
  geom_hline(yintercept = 0, show.legend = F) +
  geom_text(aes(x = sam_rf, y = y, label = comparison),
            show.legend = F,
            hjust = 0,
            data = labels
  ) +
  geom_segment(aes(x = sam_rf, xend = sam_rf, y = 0, yend = y),
               show.legend = F,
               size = 0.1,
               colour = "grey20",
               data = labels
  )
```

\svp{According to \autoref{fig:compare}, neither cross-correlation nor random forest score} fulfill both of the minimal requirements laid out \svp{in the introduction}. Both measures fulfill (R1), i.e. all  scores  of  different-source pairs are lower than scores of same-source pairs for SAM scores based on cross-correlation and RF scores.
However, SAM scores based on the multivariate random forest score show better separation between same-source and different-source scores than SAM scores based on cross-correlation. This can be seen from the larger horizontal distance between the modes (peaks) of the density curves of RF based SAM scores compared to the cross-correlation SAM scores.  

Regarding requirement (R2), \autoref{fig:compare} shows that a single cutoff value does not exist that separates same-source scores from different-source scores across all three studies without introducing errors.

There are several approaches for potential improvements: we can try to fine tune the matching algorithm to take make and model of the firearm and ammunition used into account or expand the training base of the algorithm to include a wider variety of makes and models. The downside of \svp{either of these options} is that we \svp{would} need to considerably expand the database used for training. Another solution would be to \svp{augment} the aggregation used to get from land-to-land scores to bullet scores: SAM scores only take the scores of the maximum sequence into account and ignore scores associated with pairs of land engraved areas from different sources. Those scores might be useful in establishing a baseline that better expresses similarity between pairs of bullets. One approach that uses scores for both same source pairs and different source pairs are score-based likelihood ratios \citep{Bunch:2013if, Morrison:2018fh}.

## Digging deeper: comparison of land-to-land scores

\svp{Examining land-to-land scores provides more details about the matching algorithm's performance. \autoref{fig:compare-land-to-land} shows density curves for each study. There is some overlap between known matching and known non-matching land-to-land scores; in many cases the random forest scores for these overlapping values are more extreme than the corresponding cross-correlation scores. This suggests that the matching algorithm is sensitive to the presence of LEAs which have low land-to-land match scores, such as the matches of bullets `I' and `F' in the Hamby 44 study. Comparing this to the bullet-to-bullet score density plot shown in \autoref{fig:compare}, we see that there is much greater separation between the same-source and different-source densities for each study in the bullet-to-bullet comparisons than in the land-to-land comparisons, that is, one or two poorly matching lands does not prevent the bullet from showing a matching score, though if there are weak matches on several lands, such as the marked Hamby 44 comparisons in \autoref{fig:compare}, the overall score may be affected. }

Several lands in Hamby set 44 have major deficiencies,  such as 'tank rash' (a collision of the bullet after exiting the barrel with a surface causing markings on top of the striations from the barrel) or extreme pitting (holes caused by direct contact with burning gun powder).
\autoref{fig:tankrash} gives an overview of all lands of affected bullets in the Hamby-44 set. These issues affect the algorithm's ability to identify a stable signature, which impacts the subsequent match scores \svp{ at the land-to-land and bullet-to-bullet level.}
```{r tankrash, fig.cap="Overview of bullet lands with prominent deficiency such as tank rash or extreme pitting in the Hamby-44 study.", fig.width=4, fig.height = 2, out.width='47.5%', fig.subcap=c("Br1-B1-L6 (tank rask)", "Br2-B2-L5 (tank rash)", "Br3-B1-L5 (tank rash)", "Br8-B1-L6 (tank rash)", "Br8-B2-L2 (pitting)", "Br8-B2-L6 (tank rash)", "Unk-E-L6 (tank rash)", "Unk-I-L6 (tank rash and break-off)"), fig.align="middle", fig.ncol=2}
knitr::include_graphics(
  c(
    "./images/B1-B1-L6.png", "./images/B2-B2-L5.png",
    "./images/B3-B1-L5.png", "./images/B8-B1-L6.png",
    "./images/B8-B2-L2.png", "./images/B8-B2-L6.png",
    "./images/NA-BE-L6.png", "./images/NA-BI-L6.png"
  )
)
```

Three lands from bullets known to be fired from barrel 8 are affected, producing low scores for barrel 8 in \autoref{fig:h44}. We also see that questioned bullet 'I' is affected, explaining some of the low similarity scores for this bullet.
\hh{None of the lands of any of the bullets in the other studies are affected by tank rash in a similar manner.}

```{r, cache=FALSE}
hounest <- hounest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0.05)
      d
    })
)
houlands <- hounest %>% unnest(data)

set.seed(20140501)
h44nest$samesource <- h44nest$sam_rf > 0.3
h44nest <- h44nest %>% 
  mutate(
    data = purrr::map2(.x = data, .y = samesource, .f = function(d, y) {
      d$samesource <- y
      d
  }
  ))

h44nest <- h44nest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      if (any(d$samesource)) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0.01)
      } else d$sameland <- FALSE
      d <- d %>% select(-samesource)
      d
    })
)
h44lands <- h44nest %>% unnest(data)

pdnest <- pdnest %>% mutate(
  data = data %>% purrr::map(
    .f = function(d) {
      d$sameland = bullet_to_land_predict(
        land1 = d$land1, land2 = d$land2, 
        d$rfscore, difference=0)
      d
    })
)
pdlands <- pdnest %>% unnest(data)


all_lands <- rbind(
  h44lands %>%
    select(rfscore, ccf, cms, sameland, samesource) %>%
    mutate(study = "Hamby 44", group1 = ""),
  pdlands %>%
    select(rfscore, ccf, cms, sameland, samesource) %>%
    mutate(study = "Phoenix PD", group1 = ""),
  houlands %>%
    select(rfscore, ccf, cms, sameland, samesource, group1) %>%
    mutate(study = "Houston FSI")
)
all_lands$study <- factor(all_lands$study, levels = c("Hamby 44", "Phoenix PD", "Houston FSI"))
all_lands <- all_lands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
```

```{r check44, eval=FALSE}
h44lands <- h44lands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
h44lands %>% 
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = h44lands %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)
  
```

```{r checkpd, eval=FALSE}
pdlands <- pdlands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
pdlands %>% 
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = pdlands %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)
  
```

```{r checkhou, eval=FALSE}
houlands <- houlands %>%
  mutate(
    sameland = replace(sameland, !samesource, FALSE)
  )
hou1 <- houlands %>% filter(group1==group2, group1=="G1") 

hou1 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou1 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)

hou2 <- houlands %>% filter(group1==group2, group1=="G2") 

hou2 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou2 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)  

hou3 <- houlands %>% filter(group1==group2, group1=="G3") 

hou3 %>%
  ggplot(aes(x = land1, y = land2, fill = ccf)) +
  geom_tile() +
  geom_tile(aes(colour = sameland), size = 1, 
            data = hou3 %>% filter(sameland)) +
  scale_fill_gradient2("RF score: ", midpoint=0.5, low = "darkgrey", high = "darkorange", limits = c(0, 1)) +
  scale_colour_manual(values="darkorange") +
  facet_grid(bullet1~barrel2+bullet2)  
```


```{r compare-land-to-land, fig.width = 8, fig.height = 4, fig.cap="Density curves of land-to-land similarity scores from cross-correlation (top) and RF scores (bottom). Different colors indicate same source versus different source for each land. RF scores for different source comparisons are generally well below 0.5. Some same-source comparisons are also below 0.5 for RF scores, indicating potential problems with at least one of the lands involved in the comparison."}
all_lands %>% 
  gather(key = "measure", value = "value", ccf, rfscore) %>%
  mutate(measure = factor(measure, levels = c("ccf", "rfscore"),
                          labels = c("Cross-correlation", "RF score")),
         study = paste(study, group1) %>% str_trim() %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))) %>%
  ggplot(aes(
    x = value, fill = sameland,
    colour = sameland
  )) +
  geom_density(alpha = 0.6) +
#  geom_rug(alpha = 0.5/12, show.legend = F) +
  facet_grid(measure ~ study) +
  theme_bw() +
  scale_fill_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_colour_manual("Same source", values = c("darkgrey", "darkorange")) +
  scale_x_continuous("Land-to-Land scores", limits = c(0, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) +
  theme(legend.position = "bottom") +
  geom_hline(yintercept = 0, show.legend = F)
```

When evaluating classifier performance, it is common to use a receiver operating characteristic (ROC) curve, which plots the percent of wrong identifications against the percent correct identifications for each possible value of the cutoff between the two classes (in this case, known match and known non-match). Land-to-land scores are not perfectly separated, we can therefore use ROC curves to distinguish between the performance of the different methods and studies. The ROC curves for the land-to-land scores are shown in \autoref{fig:roc-auc} (a). A perfect classifier would have 100\% correct identifications and 0\% wrong identifications, e.g. the ROC curve would be a right angle at $(0, 100)$. Classifiers with better performance will be closer to this corner of the plot. A random classifier would have an ROC curve that was a straight diagonal line through $(0,0)$ and $(100,100)$.




```{r rocs, fig.cap="ROC curves for all studies based on land-to-land comparisons of the random forest score."}
library(plotROC)
library(pROC)
rocpic <- all_lands %>% gather(measure, score, rfscore, ccf, cms) %>%
  mutate(
    measure = factor(measure, labels = c("CC", "CMS", "RF")),
    studyg = paste(study, group1) %>% str_trim %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3"))
  ) %>%
  ggplot(aes(linetype = measure, colour = studyg)) +
  geom_roc(aes(m = score, d = as.numeric(sameland)), 
           labels = FALSE, pointsize = 0, alpha = 0.8, size = 1) +
  theme_bw() +
  # coord_fixed() + 
  facet_wrap(~studyg, nrow=3) +
  xlab("Percent Wrong Identifications") +
  ylab("Percent Correct Identifications") +
  scale_colour_manual("Study", values = c("#fe9929", "#aa0000",
                                         "#74a9cf", "#2b8cbe", "#045a8d"),
                      guide=FALSE) +
  theme(legend.position= c(1, 0), legend.justification = c(1, 0), legend.direction = "vertical",
        legend.key.width = unit(1, "cm")) +
  guides(linetype = guide_legend(override.aes = list(shape = NA, size = .5))) + 
  scale_linetype_manual("Measure", values=c(1,3,2)) +
  scale_x_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "25%", "50%", "75%", "100%")) +
  scale_y_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "25%", "50%", "75%", "100%"))

```


```{r aucs-pic, fig.cap=""}
eer <- function(roc){
  dframe <- data.frame(sens = roc$sensitivities, 
                       spec = roc$specificities,
                       threshold = roc$thresholds)
  dframe$err1 <- 1 - dframe$sens
  dframe$err2 <- 1 - dframe$spec
  dframe$diff <- abs(dframe$err1-dframe$err2)
  dframe$eer <- (dframe$err1+dframe$err2)/2
  idx <- which.min(dframe$diff)
  dframe[idx,c("eer", "threshold")]
}

aucs <- all_lands %>% gather(measure, score, rfscore, ccf, cms) %>% 
  group_by(study, group1, measure) %>% nest() %>%
  mutate(
    auc = data %>% purrr::map_dbl(function(d) {
      auc(d$sameland, d$score)
    }),
    auc_ci_low = data %>% purrr::map_dbl(function(d) {
      ci.auc(d$sameland, d$score)[1]
    }),
    auc_ci_high = data %>% purrr::map_dbl(function(d) {
      ci.auc(d$sameland, d$score)[3]
    }),
    roc = data %>% purrr::map(function(d) {
      roc(d$sameland, d$score)
    }),
    errors = roc %>% purrr::map(.f = eer)
  ) %>% 
  mutate(
    studyg = paste(study, group1) %>% str_trim %>% factor(levels = c("Hamby 44", "Phoenix PD", "Houston FSI G1", "Houston FSI G2", "Houston FSI G3")),
    measure = factor(measure, levels = c("cms", "ccf", "rfscore"), labels = c("CMS", "CC", "RF")),
    measurenum = as.numeric(measure),
    measurechar = as.character(measure)
  )
aucs <- aucs %>% unnest(errors)

aucpic <- aucs %>%
  ggplot(aes(x = auc, y = measurenum, colour = studyg)) +
  facet_grid(studyg~.) +
  geom_segment(aes(x = auc_ci_low, xend = auc_ci_low, y = measurenum - .125, yend = measurenum + .125), size = 1) + 
  geom_segment(aes(x = auc_ci_high, xend = auc_ci_high, y = measurenum - .125, yend = measurenum + .125), size = 1) + 
  geom_segment(aes(x=auc_ci_low, xend=auc_ci_high, y = measurenum, yend = measurenum, linetype=measure), size=1) +
  geom_point(size=3.5, aes(shape = measure)) +
  theme_bw() +
  xlab("AUC and 95% confidence intervals") +
  xlim(c(0.5, 1)) +
  scale_y_continuous("", breaks = unique(aucs$measurenum), labels = unique(aucs$measurechar), limits=c(0.5, 3.5)) + 
  scale_colour_manual("Study", values = c("#fe9929", "#aa0000",
                                         "#74a9cf", "#2b8cbe", "#045a8d"),
                        guide = FALSE) +
  # theme(legend.position = "bottom",
  #       legend.key.width = unit(1.5, "cm")) +
  scale_linetype_manual("Measure", values = c(3,1,2), guide = F) +
  scale_shape_discrete(guide=F)

```


```{r roc-auc, echo = FALSE, out.width = c("0.51\\textwidth", ".4725\\textwidth"), out.height = c("0.63\\textwidth", "0.63\\textwidth"), fig.width = c(5, 6), fig.height = c(7.5, 7.5), fig.subcap=c("ROC curves", "Area under the curve (AUC)"), fig.cap="ROC curves and AUC for each (set) of the studies for the random forest score (RF), cross-correlation (CC) and consecutively matching striae (CMS).   As seen in the ROC curves, the algorithm performs the least well on set 2 of the Houston FSI study. Based on AUC, the overall performance on all sets is very good to excellent for cross-correlation and the random forest score. At the land level there is no significant difference in prediction power between RF score and cross-correlation.", cache=FALSE}
  rocpic
  aucpic
```

In \autoref{fig:roc-auc}a, the Houston FSI G1 and G3 curves and the Phoenix PD curve show excellent performance; Houston FSI G2 and Hamby 44 show still a very  good performance. Area under the curve (AUC) values, which summarize ROC curves, are shown in \autoref{fig:roc-auc}b. AUC values are useful for differentiating between poor, good, and excellent model performance, but are not particularly useful when determining which of several models with approximately the same level of performance should be used \citep{marzban_roc_2004}. Hamby 44 and Phoenix PD are the only studies with a somewhat noticable difference between the AUC score for RF compared to cross-correlation, but even those differences are not statistically significant.

\autoref{fig:eer} shows an overview of Equal Error and their thresholds based on the ROC curves of \autoref{fig:roc-auc}. Equal Errors are errors when sensitivity and specificity of a test are equal, i.e. we see the same percentage of missed and wrong identifications for a land-to-land comparison. Equal errors based on CMS of two lands are significantly higher than equal errors based on cross-correlation or the RF score. At best, RF score and cross-correlation have an equal error of around 5\% for land-to-land comparisons. 
Interestingly, for consecutively matching striae, equal errors are reached for just two matching striae, which from a practical point of view is not sustainable. \svp{A  CMS of  6 or above is an established cutoff used in identifications; when applied to the land-to-land comparisons, it is much more likely to result in a missed identification than a wrong identification. A threshold of 6 leads to 21 wrong identifications out of 49,286 different-source comparisons (0.04\% errors, or about 4 errors for each 10,000 evaluations); when applied to same-source comparisons, 517 of 715 same-source lands were incorrectly classified as exclusions, for an error rate of 80.6\%. }
<!-- This leads to 21 wrong identifications (out of 49286; for an error of 4 in 10,000 -- or 0.04\%) but only identifies 139 out of 715 known same-source lands, for an error of 80.6\%.  -->

```{r eval=FALSE}
xtabs(~sameland+I(round(cms)>=6), data = all_lands)
```

```{r eer, fig.width=7, fig.height = 4, fig.cap="Equal Error (in Percent)  and corresponding thresholds for all measures (RF score, Cross-correlation and CMS) under all five (sub-)studies. CMS has an equal error threshold of 2 for all studies. CMS is signifcantly worse than both Cross-correlation and the RF score. At best, Cross-correlation and RF score have equal errors for land-to-land comparison of around 5\\%.", out.width = '.8\\textwidth'}
aucs %>% 
  mutate(
    eer = 100*eer
  ) %>%
  gather(eer_thresh, value, eer, threshold) %>%
  filter(!(measure=="CMS" & eer_thresh=="threshold")) %>%
  mutate(
    eer_thresh = factor(eer_thresh, labels=c("Equal Error (in Percent)", "Threshold for Equal Error"))
  ) %>%
  ggplot(aes(y = studyg, x = value, colour=measure, shape = measure)) + 
  geom_point(size = 3.5) +
  theme_bw() +
  xlab("Equal Error (in Percent, left) and corresponding thresholds (right)") +
  ylab("") +
  scale_color_brewer(palette = "Set2") +
  scale_shape_discrete(solid = F) + 
  theme(legend.position = "bottom") +
  facet_grid(.~eer_thresh, scales="free") +
  xlim(c(0, NA))
```


# Discussion and Conclusions


What is quite surprising is that while we anticipated Hamby 44 would be the easiest set to work with, it turned out to be the hardest to solve, in part because of damage to the bullets that obscure LEA striae. It has been shown \citet{lpr} that parts of lands can be used for successful identifications. The algorithm proposed by \citet{aoas} is not capable of  automatically detecting parts of lands with well expressed striae. It may be useful to couple the \citet{aoas} algorithm with an algorithm which assesses the quality of the input data and determines which portions of the data to use for comparison. This would emulate the process used by examiners, who first assess the quality of the evidence and whether there is enough information present to attempt a match. \svp{One major disadvantage of an automated algorithm is that all of the decisions humans make (recognizing degraded land areas, excluding those areas from consideration, matching only the remaining areas) must be explicitly characterized; however, this explicit characterization means that the process can be scientifically validated to a much higher degree than the human perceptual process. }

All of the studies presented here involve the same type of firearm. We see some variations in scores across different models of Rugers, and it is probable that we will see even bigger variations with different brands of firearms that vary in number of lands and land length. We also know that different firearms and ammunition combinations mark differently well \citep{bolton-king_preventing_2016, bonfanti_influence_1999}. Rugers are among the firearms that mark very well, so we would correspondingly expect to see lowered similarity scores for other firearms. The matching algorithm works on matching lands. Barrels with polygonal rifling, such as Glocks, do not introduce well defined lands on bullets, which makes an application of the matching algorithm impossible.  


\svp{The random forest matching algorithm presented in \citet{aoas} does perform well on three different external test sets. While a single cutoff value cannot be used to distinguish matches and non-matches across the different test sets, the algorithm makes no errors when a set-specific cutoff value is used on bullet-to-bullet scores aggregated using sequence average maximum.} \svp{XXX comparison to FTEs XXX} \svp{By validating the algorithm on external test sets, we have demonstrated that the method can be generalized to different types of ammunition and is not overly sensitive to small differences in rifling procedure. Future studies can generalize this to a wider range of external test sets to establish the limits of the algorithm's generalizability.}


# References


